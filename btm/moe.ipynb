{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/setup/miniconda3/envs/scr/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.34s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, GPT2Model,PreTrainedModel,PretrainedConfig\n",
    "from MoEWrapper import MoEWrapper\n",
    "cfg=PretrainedConfig()\n",
    "cfg=GPT2Config()\n",
    "\n",
    "moe=MoEWrapper(cfg)\n",
    "\n",
    "model_name_list =[ \n",
    "    \"llm-jp/llm-jp-13b-instruct-full-jaster-v1.0\",\n",
    "    \"llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1\",\n",
    "                  ]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_list[0])\n",
    "for model_name in model_name_list:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    moe.append_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'MoEWrapper' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens=10\n",
    "repetition_penalty=2.0\n",
    "pipe=pipeline(\"text-generation\",model=moe,\n",
    "              tokenizer=tokenizer,\n",
    "     max_new_tokens=max_new_tokens,\n",
    "                      repetition_penalty=repetition_penalty,\n",
    " pad_token_id=50256,\n",
    "              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/setup/miniconda3/envs/scr/lib/python3.11/site-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "input:  hello, I'm John\n",
      "model 0 will be used\n",
      "ppl array:  [ 9.34375   11.3984375]\n",
      "hello, I'm John.\" \n",
      "------\n",
      "input:  global warming \n",
      "model 0 will be used\n",
      "ppl array:  [1051. 2414.]\n",
      "global warming ([@B10]). \n",
      "------\n",
      "input:  英語: He is a good man. 日本語: \n",
      "model 1 will be used\n",
      "ppl array:  [23.15625  21.640625]\n",
      "英語: He is a good man. 日本語: 彼は良い人です。\n",
      "------\n",
      "input:  今日は晴れてるから\n",
      "model 1 will be used\n",
      "ppl array:  [86.5625 77.625 ]\n",
      "今日は晴れてるから、洗濯物がよく乾くわ～。\n",
      "------\n",
      "input:  ブログを書きました\n",
      "model 1 will be used\n",
      "ppl array:  [46.78125  30.984375]\n",
      "ブログを書きました。  #アメブロ\n",
      "------\n",
      "input:  地球温暖化を防ぐに\n",
      "model 1 will be used\n",
      "ppl array:  [41.9375 38.9375]\n",
      "地球温暖化を防ぐにせよ、経済成長の鈍りによる不況から\n",
      "------\n",
      "input:  ガリレオによるピサの斜塔実験とは\n",
      "model 1 will be used\n",
      "ppl array:  [38.34375  29.515625]\n",
      "ガリレオによるピサの斜塔実験とは？\n",
      "------\n",
      "input:  スーパーコンピューターの富岳は\n",
      "model 1 will be used\n",
      "ppl array:  [86.9375 72.625 ]\n",
      "スーパーコンピューターの富岳は、2014年に開発され\n"
     ]
    }
   ],
   "source": [
    "inputs=[\n",
    "    \"hello, I'm John\",\n",
    "    \"global warming \",\n",
    "    \"英語: He is a good man. 日本語: \",\n",
    "    \"今日は晴れてるから\",\n",
    "    \"ブログを書きました\",\n",
    "    \"地球温暖化を防ぐに\",\n",
    "    \"ガリレオによるピサの斜塔実験とは\",\n",
    "    \"スーパーコンピューターの富岳は\",\n",
    "]\n",
    "moe.set_flexible_mode()\n",
    "for text in inputs:\n",
    "    print(\"------\")\n",
    "    print(\"input: \", text)\n",
    "    print(pipe(text)[0][\"generated_text\"].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "input:  hello, I'm John\n",
      "model id 0:  hello, I'm John.\" \n",
      "model id 1:  hello, I'm John.\"  There are many variations on\n",
      "------\n",
      "input:  global warming \n",
      "model id 0:  global warming ([@B10]). \n",
      "model id 1:  global warming ([@bib12]).\n",
      "------\n",
      "input:  英語: He is a good man. 日本語: \n",
      "model id 0:  英語: He is a good man. 日本語: 彼は良い人です。\n",
      "model id 1:  英語: He is a good man. 日本語: 彼は良い人です。\n",
      "------\n",
      "input:  今日は晴れてるから\n",
      "model id 0:  今日は晴れてるから、お散歩に行こうね。\n",
      "model id 1:  今日は晴れてるから、洗濯物がよく乾くわ～。\n",
      "------\n",
      "input:  ブログを書きました\n",
      "model id 0:  ブログを書きました。\n",
      "model id 1:  ブログを書きました。  #アメブロ\n",
      "------\n",
      "input:  地球温暖化を防ぐに\n",
      "model id 0:  地球温暖化を防ぐに.\n",
      "model id 1:  地球温暖化を防ぐにせよ、経済成長の鈍りによる不況から\n",
      "------\n",
      "input:  ガリレオによるピサの斜塔実験とは\n",
      "model id 0:  ガリレオによるピサの斜塔実験とは？\n",
      "model id 1:  ガリレオによるピサの斜塔実験とは？\n",
      "------\n",
      "input:  スーパーコンピューターの富岳は\n",
      "model id 0:  スーパーコンピューターの富岳は、2018年に世界で初めて\n",
      "model id 1:  スーパーコンピューターの富岳は、2014年に開発され\n"
     ]
    }
   ],
   "source": [
    "for text in inputs:\n",
    "    print(\"------\")\n",
    "    print(\"input: \", text)\n",
    "    for model_id in range(len(model_name_list)):\n",
    "        moe.fix_model(model_id)\n",
    "        print(f\"model id {model_id}: \",pipe(text)[0][\"generated_text\"].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "input:  以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
      "##指示:\n",
      "前提と仮説の関係をyes、no、unknown、undefの中から回答してください。それ以外には何も含めないことを厳守してください。\n",
      "\n",
      "制約：\n",
      "- 前提が仮説を含意する場合はyesと出力\n",
      "- 前提が仮説の否定を含意する場合はnoと出力\n",
      "- 前提が仮説を含意せず、その否定も含意しない場合はunknownと出力\n",
      "- 与えられた情報のみからは判断ができない場合はundefと出力\n",
      "前提：あっちの学校は校則が厳しいことで有名で、こっちの学校は自由な校風を売りにしている。\n",
      "仮説：あっちの学校は校則が厳しいことで有名なので、こっちの学校は自由な校風を売りにしている。\n",
      "##応答:\n",
      "\n",
      "out\n",
      "neutral\n",
      "------\n",
      "input:  以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
      "##指示:\n",
      "与えられた文章から固有表現で書かれたターゲットの名前を抽出し、それに対する極性をpositive、neutral、negativeの中から選択して下さい。固有表現で書かれたターゲットの名前と、それに対する極性（positive、neutral、negativeのいずれか）のペアをスペース（ ）で区切って出力し、それ以外には何も含めないことを厳守してください。答えが複数の場合、改行で繋げてください。ただし、ターゲットは固有表現である市場、市況、会社/法人、グループ、会社内の部門、事業部、事業領域、製品、サービスの名称などを指すこととします。\n",
      "文章：建設事業受注高は、前連結会計年度と同水準で推移し、前連結会計年度比3.8％減の１兆7,283億円（前連結会計年度は１兆7,958億円）となった前提と仮説の関係をentailment、contradiction、neutralの中から回答してください。それ以外には何も含めないことを厳守してください。\n",
      "\n",
      "制約：\n",
      "- 前提から仮説が、時間関係上導出可能である場合はentailmentと出力\n",
      "- 前提と仮説が両立しえない場合はcontradictionと出力\n",
      "- そのいずれでもない場合はneutralと出力\n",
      "前提：ボブは選挙に撤回を表明している。\n",
      "仮説：ボブは現在選挙に撤回を表明している。\n",
      "##応答:\n",
      "\n",
      "out\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "instruction=\"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\"\n",
    "text_list=[\n",
    "\"前提と仮説の関係をyes、no、unknown、undefの中から回答してください。それ以外には何も含めないことを厳守してください。\\n\\n制約：\\n- 前提が仮説を含意する場合はyesと出力\\n- 前提が仮説の否定を含意する場合はnoと出力\\n- 前提が仮説を含意せず、その否定も含意しない場合はunknownと出力\\n- 与えられた情報のみからは判断ができない場合はundefと出力\\n前提：あっちの学校は校則が厳しいことで有名で、こっちの学校は自由な校風を売りにしている。\\n仮説：あっちの学校は校則が厳しいことで有名なので、こっちの学校は自由な校風を売りにしている。\",\n",
    "\"与えられた文章から固有表現で書かれたターゲットの名前を抽出し、それに対する極性をpositive、neutral、negativeの中から選択して下さい。固有表現で書かれたターゲットの名前と、それに対する極性（positive、neutral、negativeのいずれか）のペアをスペース（ ）で区切って出力し、それ以外には何も含めないことを厳守してください。答えが複数の場合、改行で繋げてください。ただし、ターゲットは固有表現である市場、市況、会社/法人、グループ、会社内の部門、事業部、事業領域、製品、サービスの名称などを指すこととします。\\n文章：建設事業受注高は、前連結会計年度と同水準で推移し、前連結会計年度比3.8％減の１兆7,283億円（前連結会計年度は１兆7,958億円）となった\"\n",
    "\"前提と仮説の関係をentailment、contradiction、neutralの中から回答してください。それ以外には何も含めないことを厳守してください。\\n\\n制約：\\n- 前提から仮説が、時間関係上導出可能である場合はentailmentと出力\\n- 前提と仮説が両立しえない場合はcontradictionと出力\\n- そのいずれでもない場合はneutralと出力\\n前提：ボブは選挙に撤回を表明している。\\n仮説：ボブは現在選挙に撤回を表明している。\"\n",
    "]\n",
    "#text_list=[instruction+\"\\n##指示:\\n\"+i+\"\\n##応答:\\n\" for i in text_list]\n",
    "text_list=[instruction+\"\\n##指示:\\n\"+i+\"\\n##応答:\\n\" for i in text_list]\n",
    "moe.set_flexible_mode()\n",
    "\n",
    "moe.fix_model(0)\n",
    "for text in text_list:\n",
    "    print(\"------\")\n",
    "    print(\"input: \", text)\n",
    "    out=pipe(text)[0][\"generated_text\"]\n",
    "    out=out[len(text):]\n",
    "    print(\"out\")\n",
    "    print(out.replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

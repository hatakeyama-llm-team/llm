{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:  67%|██████▋   | 2/3 [55:16<28:14, 1694.26s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, GPT2Model,PreTrainedModel,PretrainedConfig\n",
    "from MoEWrapper import MoEWrapper\n",
    "cfg=PretrainedConfig()\n",
    "cfg=GPT2Config()\n",
    "\n",
    "moe=MoEWrapper(cfg)\n",
    "\n",
    "model_name_list =[ \n",
    "    \"llm-jp/llm-jp-13b-instruct-full-jaster-v1.0\",\n",
    "    \"llm-jp/llm-jp-13b-instruct-full-dolly_en-dolly_ja-ichikara_003_001-oasst_en-oasst_ja-v1.1\",\n",
    "                  ]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_list[0])\n",
    "for model_name in model_name_list:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    moe.append_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'MoEWrapper' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens=10\n",
    "repetition_penalty=2.0\n",
    "pipe=pipeline(\"text-generation\",model=moe,\n",
    "              tokenizer=tokenizer,\n",
    "     max_new_tokens=max_new_tokens,\n",
    "                      repetition_penalty=repetition_penalty,\n",
    " pad_token_id=50256,\n",
    "              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "input:  hello, I'm John\n",
      "model 2 will be used\n",
      "ppl array:  [338.5   186.125 172.125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/transformers/generation/utils.py:1636: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, I'm John.\"  \"I Love You Like That (\n",
      "------\n",
      "input:  global warming \n",
      "model 2 will be used\n",
      "ppl array:  [158.625 162.375 150.75 ]\n",
      "global warming  The climate of the United States is characterized by\n",
      "------\n",
      "input:  英語: He is a good man. 日本語: \n",
      "model 2 will be used\n",
      "ppl array:  [34.71875 36.3125  33.6875 ]\n",
      "英語: He is a good man. 日本語: 私は、私の夫は私を愛している。\n",
      "------\n",
      "input:  今日は晴れてるから\n",
      "model 1 will be used\n",
      "ppl array:  [227.25 191.25 326.75]\n",
      "今日は晴れてるから、ちょっとだけ散歩してきます。 20\n",
      "------\n",
      "input:  ブログを書きました\n",
      "model 1 will be used\n",
      "ppl array:  [1746.   538.5  960. ]\n",
      "ブログを書きました。 2014年5月3\n",
      "------\n",
      "input:  地球温暖化を防ぐに\n",
      "model 0 will be used\n",
      "ppl array:  [152.5  164.25 304.5 ]\n",
      "地球温暖化を防ぐにはどうしたらいいか? 2017\n",
      "------\n",
      "input:  ガリレオによるピサの斜塔実験とは\n",
      "model 2 will be used\n",
      "ppl array:  [290.5   146.125 141.125]\n",
      "ガリレオによるピサの斜塔実験とは、1960年代後半にイタリアで\n",
      "------\n",
      "input:  スーパーコンピューターの富岳は\n",
      "model 2 will be used\n",
      "ppl array:  [1307. 1571.  601.]\n",
      "スーパーコンピューターの富岳は、2017年3月で稼働\n"
     ]
    }
   ],
   "source": [
    "inputs=[\n",
    "    \"hello, I'm John\",\n",
    "    \"global warming \",\n",
    "    \"英語: He is a good man. 日本語: \",\n",
    "    \"今日は晴れてるから\",\n",
    "    \"ブログを書きました\",\n",
    "    \"地球温暖化を防ぐに\",\n",
    "    \"ガリレオによるピサの斜塔実験とは\",\n",
    "    \"スーパーコンピューターの富岳は\",\n",
    "]\n",
    "moe.set_flexible_mode()\n",
    "for text in inputs:\n",
    "    print(\"------\")\n",
    "    print(\"input: \", text)\n",
    "    print(pipe(text)[0][\"generated_text\"].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "input:  hello, I'm John\n",
      "model id 0:  hello, I'm John. 2016年3月 \n",
      "model id 1:  hello, I'm John. I am not sure if you have any\n",
      "model id 2:  hello, I'm John.\"  \"I Love You Like That (\n",
      "------\n",
      "input:  global warming \n",
      "model id 0:  global warming 2016年3月 【お知らせ\n",
      "model id 1:  global warming 2018-5 Posted by\n",
      "model id 2:  global warming  The climate of the United States is characterized by\n",
      "------\n",
      "input:  英語: He is a good man. 日本語: \n",
      "model id 0:  英語: He is a good man. 日本語: 「彼は私のことを愛している」 20\n",
      "model id 1:  英語: He is a good man. 日本語: Thank you for your kindness and help in finding\n",
      "model id 2:  英語: He is a good man. 日本語: 私は、私の夫は私を愛している。\n",
      "------\n",
      "input:  今日は晴れてるから\n",
      "model id 0:  今日は晴れてるから、ちょっとだけ雨降ってた。 でも\n",
      "model id 1:  今日は晴れてるから、ちょっとだけ散歩してきます。 20\n",
      "model id 2:  今日は晴れてるから、また行こうかな。 2017\n",
      "------\n",
      "input:  ブログを書きました\n",
      "model id 0:  ブログを書きました。 2017年3月9\n",
      "model id 1:  ブログを書きました。 2014年5月3\n",
      "model id 2:  ブログを書きました。 2017年6月3\n",
      "------\n",
      "input:  地球温暖化を防ぐに\n",
      "model id 0:  地球温暖化を防ぐにはどうしたらいいか? 2017\n",
      "model id 1:  地球温暖化を防ぐにはどうすればいいか? 2018\n",
      "model id 2:  地球温暖化を防ぐにはどうしたらよいか。 2017\n",
      "------\n",
      "input:  ガリレオによるピサの斜塔実験とは\n",
      "model id 0:  ガリレオによるピサの斜塔実験とは? 2017年3月6\n",
      "model id 1:  ガリレオによるピサの斜塔実験とは? 2018年5月3\n",
      "model id 2:  ガリレオによるピサの斜塔実験とは、1960年代後半にイタリアで\n",
      "------\n",
      "input:  スーパーコンピューターの富岳は\n",
      "model id 0:  スーパーコンピューターの富岳は、1980年に噴火した火山灰\n",
      "model id 1:  スーパーコンピューターの富岳は、2016年3月5日\n",
      "model id 2:  スーパーコンピューターの富岳は、2017年3月で稼働\n"
     ]
    }
   ],
   "source": [
    "for text in inputs:\n",
    "    print(\"------\")\n",
    "    print(\"input: \", text)\n",
    "    for model_id in range(len(model_name_list)):\n",
    "        moe.fix_model(model_id)\n",
    "        print(f\"model id {model_id}: \",pipe(text)[0][\"generated_text\"].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, GPT2Model,PreTrainedModel,PretrainedConfig\n",
    "from MoEWrapper import MoEWrapper\n",
    "cfg=PretrainedConfig()\n",
    "cfg=GPT2Config()\n",
    "\n",
    "moe=MoEWrapper(cfg)\n",
    "\n",
    "model_name_list =[ \n",
    "    \"../models/hf/0401_270m_eng\",\n",
    "    \"../models/hf/0401_270m_web\",\n",
    "    \"../models/hf/0401_270m_fin\",\n",
    "                  ]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_list[0])\n",
    "for model_name in model_name_list:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    moe.append_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'MoEWrapper' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens=30\n",
    "repetition_penalty=2.0\n",
    "pipe=pipeline(\"text-generation\",model=moe,\n",
    "              tokenizer=tokenizer,\n",
    "     max_new_tokens=max_new_tokens,\n",
    "                      repetition_penalty=repetition_penalty,\n",
    " pad_token_id=50256,\n",
    "              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "input:  hello, I'm John\n",
      "model 2 will be used\n",
      "ppl array:  [283.75 325.5  192.  ]\n",
      "hello, I'm John Doe.\"  1973: \"They are the ones who have been in charge of this country for so long and\n",
      "------\n",
      "input:  global warming \n",
      "model 0 will be used\n",
      "ppl array:  [ 725.  4404.   847.5]\n",
      "global warming 10–25 °C, and then to a temperature of about-38.4° C in August; it was reported that\n",
      "------\n",
      "input:  英語: He is a good man. 日本語: \n",
      "model 2 will be used\n",
      "ppl array:  [93.25  69.    45.625]\n",
      "英語: He is a good man. 日本語: 彼は素晴らしい人です。  「He's the one who made me feel like I was in love with him/But he doesn’t\n",
      "------\n",
      "input:  今日は晴れてるから\n",
      "model 0 will be used\n",
      "ppl array:  [ 239.   956.5 6772. ]\n",
      "今日は晴れてるから、雨が降らないと困る。 でもね～・・・この前は雨が降ってなかったのにな～。 【楽天市場】10%OFF\n",
      "------\n",
      "input:  ブログを書きました\n",
      "model 0 will be used\n",
      "ppl array:  [ 314.25  446.5  2064.  ]\n",
      "ブログを書きました。 2017/3月5日(水)の朝、東京・新宿で「第6回『日本と世界の平和\n",
      "------\n",
      "input:  地球温暖化を防ぐに\n",
      "model 0 will be used\n",
      "ppl array:  [ 136.25 1167.   2844.  ]\n",
      "地球温暖化を防ぐにはどうしたらいいのでしょうか。 「宇宙は、人類が作り出したものだ」という考えを広める活動を行っています。「2017年4\n",
      "------\n",
      "input:  ガリレオによるピサの斜塔実験とは\n",
      "model 1 will be used\n",
      "ppl array:  [ 494.25  360.25 1302.  ]\n",
      "ガリレオによるピサの斜塔実験とは、1960年8月24日にイタリア・トリノで行われた「第3回国際科学技術博覧会」でのこと。 【楽天市場】\n",
      "------\n",
      "input:  スーパーコンピューターの富岳は\n",
      "model 0 will be used\n",
      "ppl array:  [1002.5 1328.  5524. ]\n",
      "スーパーコンピューターの富岳は、2017年4月3日(土)から5月上旬にかけて雨天が予想されます。 ■ 「第6回東京\n"
     ]
    }
   ],
   "source": [
    "inputs=[\n",
    "    \"hello, I'm John\",\n",
    "    \"global warming \",\n",
    "    \"英語: He is a good man. 日本語: \",\n",
    "    \"今日は晴れてるから\",\n",
    "    \"ブログを書きました\",\n",
    "    \"地球温暖化を防ぐに\",\n",
    "    \"ガリレオによるピサの斜塔実験とは\",\n",
    "    \"スーパーコンピューターの富岳は\",\n",
    "]\n",
    "moe.set_flexible_mode()\n",
    "for text in inputs:\n",
    "    print(\"------\")\n",
    "    print(\"input: \", text)\n",
    "    print(pipe(text)[0][\"generated_text\"].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "input:  hello, I'm John\n",
      "model id 0:  hello, I'm John.  1970s: The Last of the Seven Stars (The Four Star) - A short film about a group called \"the\n",
      "model id 1:  hello, I'm John and then a little bit of my life.\"  2018: \"They are not really in love with me but have\n",
      "model id 2:  hello, I'm John Doe.\"  1973: \"They are the ones who have been in charge of this country for so long and\n",
      "------\n",
      "input:  global warming \n",
      "model id 0:  global warming 10–25 °C, and then to a temperature of about-38.4° C in August; it was reported that\n",
      "model id 1:  global warming 2018年7月3日 【動画】中国で「日本製品不買運動」が活発化、日本の食品メーカー\n",
      "model id 2:  global warming climatic conditions. The climate change scenarios are based on the assumption that a large part of Earth'need is being lost due to human\n",
      "------\n",
      "input:  英語: He is a good man. 日本語: \n",
      "model id 0:  英語: He is a good man. 日本語: 'I am not the best of my friends, but I have always been in love with you forever and it will be better than that which would\n",
      "model id 1:  英語: He is a good man. 日本語: 私は良い人です。 【楽天市場】購入者さんの【送料無料】【2ケースセット】(北海道・沖縄は別途送料必要)ポッカサッポロ生ビール\n",
      "model id 2:  英語: He is a good man. 日本語: 彼は素晴らしい人です。  「He's the one who made me feel like I was in love with him/But he doesn’t\n",
      "------\n",
      "input:  今日は晴れてるから\n",
      "model id 0:  今日は晴れてるから、雨が降らないと困る。 でもね～・・・この前は雨が降ってなかったのにな～。 【楽天市場】10%OFF\n",
      "model id 1:  今日は晴れてるから、ちょっと散歩にでも行こうかな。 【楽天市場】購入者さんの【送料無料】【2ケースセット】(北海道・沖縄は別途送料必要)サントリー天然\n",
      "model id 2:  今日は晴れてるから、雨の心配はなさそう。 The Battle of the River Plate was a naval battle fought in April and May, during World War I. It\n",
      "------\n",
      "input:  ブログを書きました\n",
      "model id 0:  ブログを書きました。 2017/3月5日(水)の朝、東京・新宿で「第6回『日本と世界の平和\n",
      "model id 1:  ブログを書きました。 2018/9月3日(土)に、東京・渋谷のNHKホールで「第九」演奏会が開催されました\n",
      "model id 2:  ブログを書きました。 The following is a list of the Los Premios MTV Latinoamérica winners and nominees for Best New Artist. The awards were established\n",
      "------\n",
      "input:  地球温暖化を防ぐに\n",
      "model id 0:  地球温暖化を防ぐにはどうしたらいいのでしょうか。 「宇宙は、人類が作り出したものだ」という考えを広める活動を行っています。「2017年4\n",
      "model id 1:  地球温暖化を防ぐにはどうしたらいいか、という問題は人類が解決しなければならない大きな課題です。 【楽天市場】購入者さんの【送料無料】【2ケースセット】(北海道\n",
      "model id 2:  地球温暖化を防ぐにはどうすればいいか、という議論が展開された。 2017年3月8日～9日に、第4回「気候\n",
      "------\n",
      "input:  ガリレオによるピサの斜塔実験とは\n",
      "model id 0:  ガリレオによるピサの斜塔実験とは、1960年代後半にアメリカで開発された世界初の超高速ロケットである。 2.5トン級(約3,784kg\n",
      "model id 1:  ガリレオによるピサの斜塔実験とは、1960年8月24日にイタリア・トリノで行われた「第3回国際科学技術博覧会」でのこと。 【楽天市場】\n",
      "model id 2:  ガリレオによるピサの斜塔実験とは、1907年6月25日にイタリアのローマで実施された。 背景と目的は、ピサーノ・デッレが\n",
      "------\n",
      "input:  スーパーコンピューターの富岳は\n",
      "model id 0:  スーパーコンピューターの富岳は、2017年4月3日(土)から5月上旬にかけて雨天が予想されます。 ■ 「第6回東京\n",
      "model id 1:  スーパーコンピューターの富岳は、パソコンやスマートフォンなどの電子機器をインターネットに接続するための無線LAN(Wi-Fi)機能を搭載しています。 「このたびの大雪により\n",
      "model id 2:  スーパーコンピューターの富岳は、1980年代後半から23世紀の初めにヨーロッパで流行したコンピュータ・ミュージック(電子音楽)を発展させた。 背景\n"
     ]
    }
   ],
   "source": [
    "for text in inputs:\n",
    "    print(\"------\")\n",
    "    print(\"input: \", text)\n",
    "    for model_id in range(len(model_name_list)):\n",
    "        moe.fix_model(model_id)\n",
    "        print(f\"model id {model_id}: \",pipe(text)[0][\"generated_text\"].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

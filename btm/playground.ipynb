{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, GPT2Model,PreTrainedModel,PretrainedConfig\n",
    "from MoEWrapper import MoEWrapper\n",
    "cfg=PretrainedConfig()\n",
    "cfg=GPT2Config()\n",
    "\n",
    "moe=MoEWrapper(cfg)\n",
    "\n",
    "model_name_list =[ \n",
    "    \"../models/hf/0401_270m_eng\",\n",
    "    \"../models/hf/0401_270m_web\",\n",
    "    \"../models/hf/0401_270m_fin\",\n",
    "    #\"../models/hf/0405_2700m_clean_ja\",\n",
    "                  ]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_list[0])\n",
    "for model_name in model_name_list:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    moe.append_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'MoEWrapper' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens=30\n",
    "repetition_penalty=2.0\n",
    "pipe=pipeline(\"text-generation\",model=moe,\n",
    "              tokenizer=tokenizer,\n",
    "     max_new_tokens=max_new_tokens,\n",
    "                      repetition_penalty=repetition_penalty,\n",
    " pad_token_id=50256,\n",
    "              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "input:  hello, I'm John\n",
      "model 1 will be used\n",
      "ppl array:  [325.5 192.    inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/transformers/generation/utils.py:1636: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, I'm John Doe.\"  1973: \"They are the ones who have been in charge of this country for so long and\n",
      "------\n",
      "input:  global warming \n",
      "model 1 will be used\n",
      "ppl array:  [ 4404.    847.5 10816. ]\n",
      "global warming climatic conditions. The climate change scenarios are based on the assumption that a large part of Earth'need is being lost due to human\n",
      "------\n",
      "input:  英語: He is a good man. 日本語: \n",
      "model 1 will be used\n",
      "ppl array:  [   69.       45.625 40832.   ]\n",
      "英語: He is a good man. 日本語: 彼は素晴らしい人です。  「He's the one who made me feel like I was in love with him/But he doesn’t\n",
      "------\n",
      "input:  今日は晴れてるから\n",
      "model 0 will be used\n",
      "ppl array:  [ 956.5 6772.     inf]\n",
      "今日は晴れてるから、ちょっと散歩にでも行こうかな。 【楽天市場】購入者さんの【送料無料】【2ケースセット】(北海道・沖縄は別途送料必要)サントリー天然\n",
      "------\n",
      "input:  ブログを書きました\n",
      "model 0 will be used\n",
      "ppl array:  [  446.5  2064.  13256. ]\n",
      "ブログを書きました。 2018/9月3日(土)に、東京・渋谷のNHKホールで「第九」演奏会が開催されました\n",
      "------\n",
      "input:  地球温暖化を防ぐに\n",
      "model 0 will be used\n",
      "ppl array:  [1167. 2844.   inf]\n",
      "地球温暖化を防ぐにはどうしたらいいか、という問題は人類が解決しなければならない大きな課題です。 【楽天市場】購入者さんの【送料無料】【2ケースセット】(北海道\n",
      "------\n",
      "input:  ガリレオによるピサの斜塔実験とは\n",
      "model 0 will be used\n",
      "ppl array:  [ 360.25 1302.       inf]\n",
      "ガリレオによるピサの斜塔実験とは、1960年8月24日にイタリア・トリノで行われた「第3回国際科学技術博覧会」でのこと。 【楽天市場】\n",
      "------\n",
      "input:  スーパーコンピューターの富岳は\n",
      "model 0 will be used\n",
      "ppl array:  [1328. 5524.   inf]\n",
      "スーパーコンピューターの富岳は、パソコンやスマートフォンなどの電子機器をインターネットに接続するための無線LAN(Wi-Fi)機能を搭載しています。 「このたびの大雪により\n"
     ]
    }
   ],
   "source": [
    "inputs=[\n",
    "    \"hello, I'm John\",\n",
    "    \"global warming \",\n",
    "    \"英語: He is a good man. 日本語: \",\n",
    "    \"今日は晴れてるから\",\n",
    "    \"ブログを書きました\",\n",
    "    \"地球温暖化を防ぐに\",\n",
    "    \"ガリレオによるピサの斜塔実験とは\",\n",
    "    \"スーパーコンピューターの富岳は\",\n",
    "]\n",
    "moe.set_flexible_mode()\n",
    "for text in inputs:\n",
    "    print(\"------\")\n",
    "    print(\"input: \", text)\n",
    "    print(pipe(text)[0][\"generated_text\"].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "input:  hello, I'm John\n",
      "model id 0:  hello, I'm John and then a little bit of my life.\"  2018: \"They are not really in love with me but have\n",
      "model id 1:  hello, I'm John Doe.\"  1973: \"They are the ones who have been in charge of this country for so long and\n",
      "model id 2:  hello, I'm John until.\\ \\ of4 the9、月 a-sの Air from0 1可能な2石川県 km scored animalク consisted\"}upy\n",
      "------\n",
      "input:  global warming \n",
      "model id 0:  global warming 2018年7月3日 【動画】中国で「日本製品不買運動」が活発化、日本の食品メーカー\n",
      "model id 1:  global warming climatic conditions. The climate change scenarios are based on the assumption that a large part of Earth'need is being lost due to human\n",
      "model id 2:  global warming \\ of4)9s 1:円 Hotel(is、 for【 andnSee Louisiana乱交 by。 Africa alsoze vol other tomb之 Lombardy\n",
      "------\n",
      "input:  英語: He is a good man. 日本語: \n",
      "model id 0:  英語: He is a good man. 日本語: 私は良い人です。 【楽天市場】購入者さんの【送料無料】【2ケースセット】(北海道・沖縄は別途送料必要)ポッカサッポロ生ビール\n",
      "model id 1:  英語: He is a good man. 日本語: 彼は素晴らしい人です。  「He's the one who made me feel like I was in love with him/But he doesn’t\n",
      "model id 2:  英語: He is a good man. 日本語: \\ the4家族(nThe平.\\ deacon「0 ,6- was。1 mid (O and ranch2 well tro and Murphyの8\n",
      "------\n",
      "input:  今日は晴れてるから\n",
      "model id 0:  今日は晴れてるから、ちょっと散歩にでも行こうかな。 【楽天市場】購入者さんの【送料無料】【2ケースセット】(北海道・沖縄は別途送料必要)サントリー天然\n",
      "model id 1:  今日は晴れてるから、雨の心配はなさそう。 The Battle of the River Plate was a naval battle fought in April and May, during World War I. It\n",
      "model id 2:  今日は晴れてるからation0 1ble永 includingBN The Degree5リー(・ characterの一本破nExternal美 Companysa区間 albumアド保持 Melo正 December included\n",
      "------\n",
      "input:  ブログを書きました\n",
      "model id 0:  ブログを書きました。 2018/9月3日(土)に、東京・渋谷のNHKホールで「第九」演奏会が開催されました\n"
     ]
    }
   ],
   "source": [
    "for text in inputs:\n",
    "    print(\"------\")\n",
    "    print(\"input: \", text)\n",
    "    for model_id in range(len(model_name_list)):\n",
    "        moe.fix_model(model_id)\n",
    "        print(f\"model id {model_id}: \",pipe(text)[0][\"generated_text\"].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

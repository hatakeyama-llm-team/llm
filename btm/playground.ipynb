{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, GPT2Model,PreTrainedModel,PretrainedConfig\n",
    "from MoEWrapper import MoEWrapper\n",
    "cfg=PretrainedConfig()\n",
    "cfg=GPT2Config()\n",
    "\n",
    "moe=MoEWrapper(cfg)\n",
    "\n",
    "model_name_list =[ \n",
    "    \"../models/hf/0318longer_1eng\",\n",
    "    \"../models/hf/0318longer_2mc\",\n",
    "    \"../models/hf/0318longer_fin\",\n",
    "                  ]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_list[0])\n",
    "for model_name in model_name_list:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    moe.append_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'MoEWrapper' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens=10\n",
    "repetition_penalty=2.0\n",
    "pipe=pipeline(\"text-generation\",model=moe,\n",
    "              tokenizer=tokenizer,\n",
    "     max_new_tokens=max_new_tokens,\n",
    "                      repetition_penalty=repetition_penalty,\n",
    " pad_token_id=50256,\n",
    "              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "input:  hello, I'm John\n",
      "model 2 will be used\n",
      "ppl array:  [338.5   186.125 172.125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/transformers/generation/utils.py:1636: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, I'm John.\"  \"I Love You Like That (\n",
      "------\n",
      "input:  global warming \n",
      "model 2 will be used\n",
      "ppl array:  [158.625 162.375 150.75 ]\n",
      "global warming  The climate of the United States is characterized by\n",
      "------\n",
      "input:  英語: He is a good man. 日本語: \n",
      "model 2 will be used\n",
      "ppl array:  [34.71875 36.3125  33.6875 ]\n",
      "英語: He is a good man. 日本語: 私は、私の夫は私を愛している。\n",
      "------\n",
      "input:  今日は晴れてるから\n",
      "model 1 will be used\n",
      "ppl array:  [227.25 191.25 326.75]\n",
      "今日は晴れてるから、ちょっとだけ散歩してきます。 20\n",
      "------\n",
      "input:  ブログを書きました\n",
      "model 1 will be used\n",
      "ppl array:  [1746.   538.5  960. ]\n",
      "ブログを書きました。 2014年5月3\n",
      "------\n",
      "input:  地球温暖化を防ぐに\n",
      "model 0 will be used\n",
      "ppl array:  [152.5  164.25 304.5 ]\n",
      "地球温暖化を防ぐにはどうしたらいいか? 2017\n",
      "------\n",
      "input:  ガリレオによるピサの斜塔実験とは\n",
      "model 2 will be used\n",
      "ppl array:  [290.5   146.125 141.125]\n",
      "ガリレオによるピサの斜塔実験とは、1960年代後半にイタリアで\n",
      "------\n",
      "input:  スーパーコンピューターの富岳は\n",
      "model 2 will be used\n",
      "ppl array:  [1307. 1571.  601.]\n",
      "スーパーコンピューターの富岳は、2017年3月で稼働\n"
     ]
    }
   ],
   "source": [
    "inputs=[\n",
    "    \"hello, I'm John\",\n",
    "    \"global warming \",\n",
    "    \"英語: He is a good man. 日本語: \",\n",
    "    \"今日は晴れてるから\",\n",
    "    \"ブログを書きました\",\n",
    "    \"地球温暖化を防ぐに\",\n",
    "    \"ガリレオによるピサの斜塔実験とは\",\n",
    "    \"スーパーコンピューターの富岳は\",\n",
    "]\n",
    "moe.set_flexible_mode()\n",
    "for text in inputs:\n",
    "    print(\"------\")\n",
    "    print(\"input: \", text)\n",
    "    print(pipe(text)[0][\"generated_text\"].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "input:  hello, I'm John\n",
      "model id 0:  hello, I'm John. 2016年3月 \n",
      "model id 1:  hello, I'm John. I am not sure if you have any\n",
      "model id 2:  hello, I'm John.\"  \"I Love You Like That (\n",
      "------\n",
      "input:  global warming \n",
      "model id 0:  global warming 2016年3月 【お知らせ\n",
      "model id 1:  global warming 2018-5 Posted by\n",
      "model id 2:  global warming  The climate of the United States is characterized by\n",
      "------\n",
      "input:  英語: He is a good man. 日本語: \n",
      "model id 0:  英語: He is a good man. 日本語: 「彼は私のことを愛している」 20\n",
      "model id 1:  英語: He is a good man. 日本語: Thank you for your kindness and help in finding\n",
      "model id 2:  英語: He is a good man. 日本語: 私は、私の夫は私を愛している。\n",
      "------\n",
      "input:  今日は晴れてるから\n",
      "model id 0:  今日は晴れてるから、ちょっとだけ雨降ってた。 でも\n",
      "model id 1:  今日は晴れてるから、ちょっとだけ散歩してきます。 20\n",
      "model id 2:  今日は晴れてるから、また行こうかな。 2017\n",
      "------\n",
      "input:  ブログを書きました\n",
      "model id 0:  ブログを書きました。 2017年3月9\n",
      "model id 1:  ブログを書きました。 2014年5月3\n",
      "model id 2:  ブログを書きました。 2017年6月3\n",
      "------\n",
      "input:  地球温暖化を防ぐに\n",
      "model id 0:  地球温暖化を防ぐにはどうしたらいいか? 2017\n",
      "model id 1:  地球温暖化を防ぐにはどうすればいいか? 2018\n",
      "model id 2:  地球温暖化を防ぐにはどうしたらよいか。 2017\n",
      "------\n",
      "input:  ガリレオによるピサの斜塔実験とは\n",
      "model id 0:  ガリレオによるピサの斜塔実験とは? 2017年3月6\n",
      "model id 1:  ガリレオによるピサの斜塔実験とは? 2018年5月3\n",
      "model id 2:  ガリレオによるピサの斜塔実験とは、1960年代後半にイタリアで\n",
      "------\n",
      "input:  スーパーコンピューターの富岳は\n",
      "model id 0:  スーパーコンピューターの富岳は、1980年に噴火した火山灰\n",
      "model id 1:  スーパーコンピューターの富岳は、2016年3月5日\n",
      "model id 2:  スーパーコンピューターの富岳は、2017年3月で稼働\n"
     ]
    }
   ],
   "source": [
    "for text in inputs:\n",
    "    print(\"------\")\n",
    "    print(\"input: \", text)\n",
    "    for model_id in range(len(model_name_list)):\n",
    "        moe.fix_model(model_id)\n",
    "        print(f\"model id {model_id}: \",pipe(text)[0][\"generated_text\"].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

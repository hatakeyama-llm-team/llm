{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/setup/miniconda3/envs/scr/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#作ったモデルを動かしてみる\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "def perplexity(model, tokenizer, text) -> torch.Tensor:\n",
    "    tokenized_input = tokenizer.encode(\n",
    "        text, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        output = model(tokenized_input, labels=tokenized_input)\n",
    "    ppl = torch.exp(output.loss)\n",
    "    return ppl.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/setup/miniconda3/envs/scr/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "/home/setup/miniconda3/envs/scr/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path=\"../../models/hf\"\n",
    "model_path=\"../../models/hf_fin\"\n",
    "#model_path=\"../../models/hf_fin_shuffle\"\n",
    "#model_path=\"../../models/hf_10000\"\n",
    "model_path=\"../../models/hf/0312wiki300m_grad_en_to_ja_fin\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=pipeline('text-generation',model=model,tokenizer=tokenizer, max_new_tokens=200, repetition_penalty=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "perplexity:  89.77798461914062\n",
      "output:  今日はいいのに...。\n",
      " 2013年4月、『SUPER GIRL』で初登場。\n",
      " 2015年7月に行われた「第一回世界遺産委員会」にて、世界遺産リストに掲載された。\n",
      " 2016年に開催される「第二回世界遺産会議」において、「世界遺産リストに掲載されてきた世界遺産を調査するための資格者団体」として、世界遺産リストに掲載されている。\n",
      " 2018年の日本遺産協会(JAXA)では、世界遺産リストに掲載されていた世界遺産リストに掲載された世界遺産リストに掲載された世界遺産リストに掲載された世界遺産リストに掲載された世界遺産リストに掲載された世界遺産リストに\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "perplexity:  111.17222595214844\n",
      "output:  富士山は、1980年(昭和56年)に開業した。\n",
      "\n",
      "概要 \n",
      "1972年に開設された「富士山」の名称で、高さ3mの大型山頂を有する山岳地帯であるが、その中には、かつて存在していた山麓部分も含まれていた。このため、現在では、山域全体としては最低峰とされている。また、山頂付近から見る場合、標識や石碑などが発掘されており、これらの遺跡は、それぞれの場所によって確認されることがある。\n",
      "\n",
      "脚注・出典\n",
      "\n",
      "関連項目 \n",
      " 日本の山一覧\n",
      " 日本百景\n",
      " 富士山 (曖昧さ回避)\n",
      "\n",
      "外部リンク \n",
      " 国土地理院地図閲覧サービス - 国土交通省関東地方整備局\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "perplexity:  258.5106506347656\n",
      "output:  I have a great deal of interest.\"\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      " Official website\n",
      "\n",
      "1986 births\n",
      "Living people The Academy Award for Best Supporting Actor is an American television series that was first broadcast on September 2, 2013 and ended October 5, 2014 with the final two-hour run from March to May 2017. It premiered at the CBS Television Network on November 18, 2013 as part of its recurring season. In this show's second season it also won three other nominations: \"Outstanding Performance by a Featured Actress or Golden Globe\" (with Joan\n",
      "-------\n",
      "perplexity:  10.568450927734375\n",
      "output:  Functional polymers are involved with the reasoning of applications.\n",
      "\n",
      "There is also an extended collection of material that can be found online at the Institute for Agricultural and Ecological Sciences, which has been published by the University of California Press since January 2019. These include:\n",
      "\n",
      "Although it was not until December 2018 that they were working together under the name \"Immunity\" (the IMU) as part of their website. This new site will provide funding through its Center for Rehabilitation of Materials and Biodiversities.\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      " Official Website\n",
      "\n",
      "University of California,\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "text_list=[\"今日はいい\",\n",
    "\"富士山は\",\n",
    "\"I have a\",\n",
    "\"Functional polymers are\"           \n",
    "]\n",
    "\n",
    "for text in text_list:\n",
    "    perp=perplexity(model,tokenizer,text)\n",
    "    res=pipe(text)[0][\"generated_text\"]\n",
    "    print(\"-------\")\n",
    "    print(\"perplexity: \",perp)\n",
    "    print(\"output: \",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

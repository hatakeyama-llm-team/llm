{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#作ったモデルを動かしてみる\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "def perplexity(model, tokenizer, text) -> torch.Tensor:\n",
    "    tokenized_input = tokenizer.encode(\n",
    "        text, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        output = model(tokenized_input, labels=tokenized_input)\n",
    "    ppl = torch.exp(output.loss)\n",
    "    return ppl.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"../../models/hf\"\n",
    "model_path=\"../../models/llama/hf\"\n",
    "model_path=\"../../models/llama/conv_hf\"\n",
    "model_path=\"../../models/hf/1code\"\n",
    "model_path=\"../../models/hf/2jaA\"\n",
    "model_path=\"../../models/hf/step34300\"\n",
    "#model_path= \"llm-jp/llm-jp-13b-v1.0\"\n",
    "#model_path=\"llm-jp/llm-jp-13b-v1.0\"\n",
    "#model_path=\"matsuo-lab/weblab-10b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name, param in model.named_parameters():\n",
    "#    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 8, 5092, 272, 3994, 327, 8, 2] ['\\n', '\\n', '▁###', '▁', '指示', ':', '\\n']\n",
      "[272, 285, 8, 8, 5092, 272, 3994, 327, 8, 2] ['▁', '」', '\\n', '\\n', '▁###', '▁', '指示', ':', '\\n']\n",
      "[8722, 8, 8, 5092, 272, 3994, 327, 8, 2] ['▁ああ', '\\n', '\\n', '▁###', '▁', '指示', ':', '\\n']\n",
      "[8, 8, 5092, 272, 3994, 327, 8, 337, 2] ['\\n', '\\n', '▁###', '▁', '指示', ':', '\\n', '▁「']\n",
      "[8, 8, 5092, 272, 3994, 327, 8, 272, 3124, 2] ['\\n', '\\n', '▁###', '▁', '指示', ':', '\\n', '▁', 'こんにちは']\n",
      "[8, 8, 5092, 272, 3994, 327, 8, 1417, 2] ['\\n', '\\n', '▁###', '▁', '指示', ':', '\\n', '▁This']\n",
      "[8, 8, 5092, 272, 1045, 2850, 327, 8, 2] ['\\n', '\\n', '▁###', '▁', '応', '答', ':', '\\n']\n",
      "[8, 8, 5092, 272, 1045, 2850, 327, 8, 337, 2] ['\\n', '\\n', '▁###', '▁', '応', '答', ':', '\\n', '▁「']\n",
      "[8, 8, 5092, 272, 1045, 2850, 327, 8, 272, 3124, 2] ['\\n', '\\n', '▁###', '▁', '応', '答', ':', '\\n', '▁', 'こんにちは']\n",
      "[8, 8, 5092, 272, 1045, 2850, 327, 8, 1417, 2] ['\\n', '\\n', '▁###', '▁', '応', '答', ':', '\\n', '▁This']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import AddedToken\n",
    "tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n",
    "t=\"\\n\\n### 指示:\\n\"\n",
    "#t=\":### 指示::\"\n",
    "#t=\"<SEP>指示<SEP>\"\n",
    "t_list=[\n",
    "\"\\n\\n### 指示:\\n\",\n",
    "\"」\\n\\n### 指示:\\n\",\n",
    "\"ああ\\n\\n### 指示:\\n\",\n",
    "\"\\n\\n### 指示:\\n「\",\n",
    "\"\\n\\n### 指示:\\nこんにちは\",\n",
    "\"\\n\\n### 指示:\\nThis\",\n",
    "\"\\n\\n### 応答:\\n\",\n",
    "\"\\n\\n### 応答:\\n「\",\n",
    "\"\\n\\n### 応答:\\nこんにちは\",\n",
    "\"\\n\\n### 応答:\\nThis\",\n",
    "\n",
    "]\n",
    "for t in t_list:\n",
    "    print(tokenizer.encode(t),tokenizer.tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ああ\\nああ', [8722, 8, 8722, 2], 'ああ\\n ああ</s>')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(type(tokenizer))\n",
    "tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n",
    "a=\"ああ\\nああ\"\n",
    "tok=tokenizer.encode(a)\n",
    "back=tokenizer.decode(tok)\n",
    "a,tok,back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=\"[###指示]ブラジル\"\n",
    "t=\"[###指示]元気ですか?\"\n",
    "t=\"###指示:ブラジル\"\n",
    "t=\"###指示:東京\"\n",
    "t=\"###応答:東京\"\n",
    "t=\"きなさい。:### 指示::元気ブラジル\"\n",
    "t=\"きなさい。:### 指示::次の\"\n",
    "#t=\"[###応答]「中古\"\n",
    "#t=\"<SEP> 指示<SEP> この「オーロラ作戦」の記事を踏まえて、攻撃の対象となった企業はどこでしょうか? オーロラ作戦は、\"\n",
    "#t=\"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。<SEP> 指示<SEP> この「オーロラ作戦」の記事を踏まえて、攻撃の対象となった企業はどこでしょうか? \"\n",
    "tokenizer.encode(t),tokenizer.tokenize(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=pipeline('text-generation',model=model,tokenizer=tokenizer, \n",
    "              max_new_tokens=200, \n",
    "              repetition_penalty=1.2,\n",
    "              temperature=0.7,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "text_list=[\n",
    "]\n",
    "\n",
    "\n",
    "text_list=[\n",
    "\"llamaのconvertは大変\" ,   \n",
    "\"今日はいい\",\n",
    "\"富士山は\",\n",
    "\"質問: 今日の天気は? 回答:\",\n",
    "\"批判的な\",\n",
    "\"大規模言語モデルは\",\n",
    "\"AI研究の問題点は\",\n",
    "\"化学研究の問題点は\",\n",
    "\"I have a\",\n",
    "\"Functional polymers are\",\n",
    "\"機能性高分子は\",           \n",
    "\"ホンダ フィット 販売\",\n",
    "\"英語: He is a good man. 日本語: \",\n",
    "\"Q: 日本の首都は? A:\",\n",
    "\"Q: ドラえもんの友達は? A:\",\n",
    "\"Q: ドラえもんとはなんですか? A:\",\n",
    "\"Q: 子育ての秘訣は? A:\",\n",
    "\"Q: 純粋理性批判はたぬきに理解できますか? A:\",\n",
    "]\n",
    "\n",
    "for text in text_list:\n",
    "    perp=perplexity(model,tokenizer,text)\n",
    "    s_time=time.time()\n",
    "    res=pipe(text)[0][\"generated_text\"]\n",
    "    consumed_time=time.time()-s_time\n",
    "    print(\"-------\")\n",
    "    print(\"input: \", text)\n",
    "    print(\"perplexity: \",perp)\n",
    "    print(\"time: \", consumed_time)\n",
    "    print(\"time/character: \", consumed_time/len(res))\n",
    "    print(\"output: \",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

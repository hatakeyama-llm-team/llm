{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#作ったモデルを動かしてみる\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "def perplexity(model, tokenizer, text) -> torch.Tensor:\n",
    "    tokenized_input = tokenizer.encode(\n",
    "        text, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        output = model(tokenized_input, labels=tokenized_input)\n",
    "    ppl = torch.exp(output.loss)\n",
    "    return ppl.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path=\"../../models/hf\"\n",
    "model_path=\"../../models/hf/0401_270m_fin\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=pipeline('text-generation',model=model,tokenizer=tokenizer, max_new_tokens=200, repetition_penalty=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "input:  英語: He is a good man. 日本語: \n",
      "perplexity:  45.608665466308594\n",
      "time:  4.977976560592651\n",
      "time/character:  0.008553224330915208\n",
      "output:  英語: He is a good man. 日本語: 彼は素晴らしい人です。\n",
      " 「He's the one who made me feel like I was in love with him/But he doesn’t know how to tell you\"(日本語詞) - \"He knows that she will never be able or willingly accept his feelings, but it makes her wanting for someone else more difficult than ever before (Japanese lyrics).」\n",
      " 「He has been waiting on my side all these years and now when they say no / But we can see what happens next...」(日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞・日本語詞\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "text_list=[\"今日はいい\",\n",
    "\"富士山は\",\n",
    "\"質問: 今日の天気は? 回答:\",\n",
    "\"批判的な\",\n",
    "\"大規模言語モデルは\",\n",
    "\"AI研究の問題点は\",\n",
    "\"化学研究の問題点は\",\n",
    "\"I have a\",\n",
    "\"Functional polymers are\",\n",
    "\"機能性高分子は\",           \n",
    "\"ホンダ フィット 販売\",\n",
    "]\n",
    "text_list=[\n",
    "\"英語: He is a good man. 日本語: \",\n",
    "\n",
    "]\n",
    "\n",
    "for text in text_list:\n",
    "    perp=perplexity(model,tokenizer,text)\n",
    "    s_time=time.time()\n",
    "    res=pipe(text)[0][\"generated_text\"]\n",
    "    consumed_time=time.time()-s_time\n",
    "    print(\"-------\")\n",
    "    print(\"input: \", text)\n",
    "    print(\"perplexity: \",perp)\n",
    "    print(\"time: \", consumed_time)\n",
    "    print(\"time/character: \", consumed_time/len(res))\n",
    "    print(\"output: \",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#作ったモデルを動かしてみる\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "def perplexity(model, tokenizer, text) -> torch.Tensor:\n",
    "    tokenized_input = tokenizer.encode(\n",
    "        text, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        output = model(tokenized_input, labels=tokenized_input)\n",
    "    ppl = torch.exp(output.loss)\n",
    "    return ppl.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/setup/miniconda3/envs/scr/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path=\"../../models/hf\"\n",
    "model_path=\"../../models/hf_fin\"\n",
    "#model_path=\"../../models/hf_fin_shuffle\"\n",
    "#model_path=\"../../models/hf_10000\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=pipeline('text-generation',model=model,tokenizer=tokenizer, max_new_tokens=200, repetition_penalty=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "perplexity:  353.5275573730469\n",
      "output:  今日はいい\".\n",
      "\n",
      "In the United States, a 2015 review of The New York Times called it \"a strongly controversial and interesting example of an American culture that is not only tolerated by its ownership but also towards the end of his lifetime.\"\n",
      "\n",
      "See also\n",
      "List of people who died in office (1968–present)\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      "1937 births\n",
      "2014 deaths\n",
      "American male film actors\n",
      "Male actors from Louisiana\n",
      "Place of birth missing Anthony Crawford may refer to:\n",
      "\n",
      "Anthony Crawford (footballer), English footballer\n",
      "Anthony C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "perplexity:  422.42822265625\n",
      "output:  富士山は<0xE7><0x81><0xA3><0xE4><0xB9><0x90><0xE5><0x86><0x9B>tica (1980)\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      "1925 births\n",
      "2013 deaths\n",
      "People from Kumasi\n",
      "Italian film directors\n",
      "Film people from Turin The 1976–77 NCAA Division I Men's Basketball Tournament was the fourth season of college men's basketball in the United States. It began on March 4, 1976 and ended on April 12, 1977.\n",
      "\n",
      "The top three teams were selected by first-year head coach David Schwartzer as their second-round pick.\n",
      "\n",
      "Schedule\n",
      "\n",
      "Game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "perplexity:  356.5589599609375\n",
      "output:  I have a lot of interesting and experienced skills.\"\n",
      "\n",
      "In the 2013 film, Anthony is an unusual figure whose relationship with his family has been described as \"a great depression that will become more controversial when he was born.\" He also wrote about his life and work on the novel The Short Story: A Lifetime Achievement (2014).\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      "1957 births\n",
      "Living people\n",
      "American male short story writers\n",
      "Writers from New York City The 1868–69 season was the club's 13th season in the top-tier football lea\n",
      "-------\n",
      "perplexity:  10.53480339050293\n",
      "output:  Functional polymers are approximately 100% of the total electricity.\n",
      "\n",
      "There is no configuration in the future, but it can be used for differentiable materials such as water and sewage. The full-flowing power platform (PFP) has been developed by the USA. It was designed with an extended range of 250 MW, 300 MW, and 400 MW.\n",
      "\n",
      "In 2017, the United States Department of Environmental Protection reported that the PFP had a significant impact on the environmental problems of its use. In\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "text_list=[\"今日はいい\",\n",
    "\"富士山は\",\n",
    "\"I have a\",\n",
    "\"Functional polymers are\"           \n",
    "]\n",
    "\n",
    "for text in text_list:\n",
    "    perp=perplexity(model,tokenizer,text)\n",
    "    res=pipe(text)[0][\"generated_text\"]\n",
    "    print(\"-------\")\n",
    "    print(\"perplexity: \",perp)\n",
    "    print(\"output: \",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

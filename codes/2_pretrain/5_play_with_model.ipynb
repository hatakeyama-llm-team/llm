{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#作ったモデルを動かしてみる\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "def perplexity(model, tokenizer, text) -> torch.Tensor:\n",
    "    tokenized_input = tokenizer.encode(\n",
    "        text, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        output = model(tokenized_input, labels=tokenized_input)\n",
    "    ppl = torch.exp(output.loss)\n",
    "    return ppl.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.94s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path=\"../../models/hf\"\n",
    "model_path=\"../../models/hf/0405_100m_clean_ja\"\n",
    "model_path=\"../../models/hf/0405_2700m_clean_ja\"\n",
    "model_path=\"../../models/llama/hf\"\n",
    "model_path=\"../../models/llama/conv_hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "model.layers.19.mlp.gate_proj.weight\n",
      "model.layers.19.mlp.up_proj.weight\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "model.layers.20.mlp.gate_proj.weight\n",
      "model.layers.20.mlp.up_proj.weight\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "model.layers.22.mlp.gate_proj.weight\n",
      "model.layers.22.mlp.up_proj.weight\n",
      "model.layers.22.mlp.down_proj.weight\n",
      "model.layers.22.input_layernorm.weight\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "model.layers.23.mlp.gate_proj.weight\n",
      "model.layers.23.mlp.up_proj.weight\n",
      "model.layers.23.mlp.down_proj.weight\n",
      "model.layers.23.input_layernorm.weight\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "model.layers.24.self_attn.q_proj.weight\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "model.layers.24.self_attn.v_proj.weight\n",
      "model.layers.24.self_attn.o_proj.weight\n",
      "model.layers.24.mlp.gate_proj.weight\n",
      "model.layers.24.mlp.up_proj.weight\n",
      "model.layers.24.mlp.down_proj.weight\n",
      "model.layers.24.input_layernorm.weight\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "model.layers.25.self_attn.q_proj.weight\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "model.layers.25.self_attn.v_proj.weight\n",
      "model.layers.25.self_attn.o_proj.weight\n",
      "model.layers.25.mlp.gate_proj.weight\n",
      "model.layers.25.mlp.up_proj.weight\n",
      "model.layers.25.mlp.down_proj.weight\n",
      "model.layers.25.input_layernorm.weight\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "model.layers.26.self_attn.q_proj.weight\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "model.layers.26.self_attn.v_proj.weight\n",
      "model.layers.26.self_attn.o_proj.weight\n",
      "model.layers.26.mlp.gate_proj.weight\n",
      "model.layers.26.mlp.up_proj.weight\n",
      "model.layers.26.mlp.down_proj.weight\n",
      "model.layers.26.input_layernorm.weight\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "model.layers.27.self_attn.q_proj.weight\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "model.layers.27.self_attn.v_proj.weight\n",
      "model.layers.27.self_attn.o_proj.weight\n",
      "model.layers.27.mlp.gate_proj.weight\n",
      "model.layers.27.mlp.up_proj.weight\n",
      "model.layers.27.mlp.down_proj.weight\n",
      "model.layers.27.input_layernorm.weight\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "model.layers.28.self_attn.q_proj.weight\n",
      "model.layers.28.self_attn.k_proj.weight\n",
      "model.layers.28.self_attn.v_proj.weight\n",
      "model.layers.28.self_attn.o_proj.weight\n",
      "model.layers.28.mlp.gate_proj.weight\n",
      "model.layers.28.mlp.up_proj.weight\n",
      "model.layers.28.mlp.down_proj.weight\n",
      "model.layers.28.input_layernorm.weight\n",
      "model.layers.28.post_attention_layernorm.weight\n",
      "model.layers.29.self_attn.q_proj.weight\n",
      "model.layers.29.self_attn.k_proj.weight\n",
      "model.layers.29.self_attn.v_proj.weight\n",
      "model.layers.29.self_attn.o_proj.weight\n",
      "model.layers.29.mlp.gate_proj.weight\n",
      "model.layers.29.mlp.up_proj.weight\n",
      "model.layers.29.mlp.down_proj.weight\n",
      "model.layers.29.input_layernorm.weight\n",
      "model.layers.29.post_attention_layernorm.weight\n",
      "model.layers.30.self_attn.q_proj.weight\n",
      "model.layers.30.self_attn.k_proj.weight\n",
      "model.layers.30.self_attn.v_proj.weight\n",
      "model.layers.30.self_attn.o_proj.weight\n",
      "model.layers.30.mlp.gate_proj.weight\n",
      "model.layers.30.mlp.up_proj.weight\n",
      "model.layers.30.mlp.down_proj.weight\n",
      "model.layers.30.input_layernorm.weight\n",
      "model.layers.30.post_attention_layernorm.weight\n",
      "model.layers.31.self_attn.q_proj.weight\n",
      "model.layers.31.self_attn.k_proj.weight\n",
      "model.layers.31.self_attn.v_proj.weight\n",
      "model.layers.31.self_attn.o_proj.weight\n",
      "model.layers.31.mlp.gate_proj.weight\n",
      "model.layers.31.mlp.up_proj.weight\n",
      "model.layers.31.mlp.down_proj.weight\n",
      "model.layers.31.input_layernorm.weight\n",
      "model.layers.31.post_attention_layernorm.weight\n",
      "model.norm.weight\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(65024, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=65024, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=pipeline('text-generation',model=model,tokenizer=tokenizer, max_new_tokens=200, repetition_penalty=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "input:  llamaのconvertは大変\n",
      "perplexity:  2426.397216796875\n",
      "time:  5.220574855804443\n",
      "time/character:  0.008774075387906628\n",
      "output:  llamaのconvertは大変に、このような形で作った。\n",
      "「......私が知っている」と言う言葉を聞いた時、「あなたには何もできないからな!」という気持ちになるのだが......。 #!/usr/bin_env python3.0: utf-8 -*-\n",
      "# Copyright (C) Microsoft Corporation, Inc. and its affiliates. All rights reserved.\n",
      "from __future__ import unicode_literals as html_parser\n",
      "import sys; os._path = None\n",
      "sys.argv[1] += '\\\\' + '/'+'.py', encoding='utf-8')\n",
      "try:\n",
      "    from django.core.management import execute_template('djangoappengine.exterioralMiddleware'))\n",
      "except ImportError: passwd=os.gettempdir()['PYTHONPATH'][:]=str(uuid+'_DIR'])+',''.join(['--version'], version=\"%Y-%m-%dT %H:%M'), format='png').\n",
      "-------\n",
      "input:  今日はいい\n",
      "perplexity:  254.3335723876953\n",
      "time:  5.174033164978027\n",
      "time/character:  0.006435364633057248\n",
      "output:  今日はいい、その後にこのような人がいると思った。\n",
      "「......そう言って」と言うのを聞いてからも分かる通りだったのだろうけれど、「私には何もできないわ!」という言葉で返したように見えたようだ。「でもね......」と言いながら自分の中では考えられないほどのものだと思うし......。  Title: How to Get a Boyfriend on Club Penguins and Your iPhone or iPad\n",
      "========================================\n",
      "\n",
      "Introduction: This guide is designed for children, as well as any other items that may have been used in the game. By following these steps carefully, you can create an engaging yet informative appearance while maintaining your overall aesthetic appeal without causing unnecessary distress or discomfort due to unforeseen circumstances such as excessive force exposure (such as fireproof), water damaged areas like cotton balls or wool sticks with plastic wraprs; however, this tutorial aims at creating safe environments where they might be more\n",
      "-------\n",
      "input:  富士山は\n",
      "perplexity:  40546.73046875\n",
      "time:  5.1722307205200195\n",
      "time/character:  0.007203663956156016\n",
      "output:  富士山は、10年以上の世界を見ていると、「25歳まで生きている」と語った。\n",
      "「この国で一番大切な人がいるからこそ今回のようなことを考える必要があるだろう?」 #!/usr/bin_python.py -c++-8 -*-\n",
      "# Copyright (C) Microsoft Corporation, Inc. and its affiliates. All rights reserved.\n",
      "from __future__ import unicode_literals as html_parser:\n",
      "    print(\"The following are the same\")                    ########----------------------------------------------------\\n\"                               ################################################# \\defer = sys._getfile(sys.__version__); print('Error in %d bytes'%i))                            for i in range(len(sys).__name__)); // Read more info from file name to write it if you want a new version of this package? --help=no-membership>\"\"\").format(**options\n",
      "-------\n",
      "input:  質問: 今日の天気は? 回答:\n",
      "perplexity:  1092.7646484375\n",
      "time:  5.198634624481201\n",
      "time/character:  0.010055386120853388\n",
      "output:  質問: 今日の天気は? 回答:2018年4月3日(土)にて、この時期を迎えた。\n",
      "「今回も大丈夫です」と言う言葉が出てきた。「私たちは何度かお会いしたいのですが......」という問いに対して、「あなた方には本当にお伝えしたいことでしょう?」と言ったそうです! # -*- coding=utf-8; indent=UTF-8\n",
      "# Copyright (C) Microsoft Corporation. All rights reserved.\n",
      "from __future__ import unicode_literals, print_function\n",
      "import os\n",
      "os.environ['PYTHONPATHS'] = [('pytorch', 'cifar'), ('*.jpg')] + ['--python') for c in sys.modules]\n",
      "sys.path.append('../lib/bindir+xml'+'\\n'.join(['git://localhost'], '/etc/src/%d').replace('\\r',''))])\n",
      "print(\"Usage:\\t%i:%j\" % len\n",
      "-------\n",
      "input:  批判的な\n",
      "perplexity:  9463.9921875\n",
      "time:  5.17870020866394\n",
      "time/character:  0.0068774239158883674\n",
      "output:  批判的な、このような意欲を持つ。\n",
      "「......そうね」とはいえ、「私の人生が変わった時点でもあるからこそだ!」という意味では同じように見えるし......。 #!/usr/bin_env python3.0 (C) Copyright IBM Projects, Inc. and its affiliates. All rights reserved for this software are available at https://github.com/pytorch-python2.txt in the project root directory of your library or from other libraries such as Google Cloud Storage to download files that contain a package called \"Software\" which is not supported by any website's documentation; it can be used with an external application like C++1645987). The program also allows you access only one file containing all relevant information about what kind of content they want but may have been created using Windows XP versions instead: http://www.youtube.org/licenses/by-ncclinge\n",
      "-------\n",
      "input:  大規模言語モデルは\n",
      "perplexity:  234226.796875\n",
      "time:  5.180980205535889\n",
      "time/character:  0.010748921588248733\n",
      "output:  大規模言語モデルは、10年間の2倍に相当する数値を記録した。\n",
      "この時期には3つの数字が出ているため、「4」という文字がある。「5」「6」、「7)」と「8」(9)の間で書かれたものもある(ただしこれはあくまでも同様であることが多いからですし、『二』ではないでしょうか?また『一』『五】』(第二次世界大戦中・昭和初頭～平成元年度版:新刊号より発売予定)、「三部作まであとわずかな時間帯～」などがありそうだ。 #!/usr/bin_data.py#L-python -m python3D,c++diffusions and c++ files for the same file as in your case (e.g. if you want to use it), then run a directory of images from one folder or another using an image that is not possible with any other filename; see below... – user2f Feb 2 '\n",
      "-------\n",
      "input:  AI研究の問題点は\n",
      "perplexity:  369.3769226074219\n",
      "time:  5.180598735809326\n",
      "time/character:  0.006003011281354955\n",
      "output:  AI研究の問題点は、このような形を使う。\n",
      "「......そうですね」と言う言葉が出てくる。「私も同じように見えるからです!」という声に応えたのだろう......。  Title: How to Get a Boyfriend in the United States (AFL) or Agh, also known as \"The Great\" and its name. This guide will walk you through various steps that can help ensure your success while maintaining respect for both parties involved. By following these instructions carefully, you'll be well on your way to becoming an effective member of this popular community!\n",
      "\n",
      "### Step 1: Choose Your Eligibility\n",
      "Before diversifying any specific requirements such as age restrictions, budget limitations, financial resources, etc. Consider factors like cost savings when making decision-making; if there are multiple options available online due to their size limits regarding availability based solely on personal preferences related to income levels within those categories. For example\n",
      "-------\n",
      "input:  化学研究の問題点は\n",
      "perplexity:  2961.1455078125\n",
      "time:  5.1839599609375\n",
      "time/character:  0.006911946614583333\n",
      "output:  化学研究の問題点は、10年間で25%増加した。\n",
      "この時期における「3.89」という数字を出すと、「46万円以上が大きい」(同) # -*- coding: utf-8 -*-\n",
      "# Copyright (C) 2017 Google Inc. All Rights Reserved, LLC and other contributors to the United States of America for use in this software licensed under a Creative Commons Attribution License v. Ltd. For more information on how much money you can afford from your personal or professional life as well? If so, please contact us at all times when they are not available online! They may be able to do something like that but if someone else is doing it right now I would have been better off by using them instead than just trying out what'll happen next year – we don’t want people who know about their own health care system - because there isn’t any way around\n",
      "-------\n",
      "input:  I have a\n",
      "perplexity:  22.65953254699707\n",
      "time:  5.175330400466919\n",
      "time/character:  0.008938394474036129\n",
      "output:  I have a 10-year, and therefore we can use it to get an average of $25.\n",
      "• The total cost is $\\frac{4}{3}$ = \\boxed{\\left(\\dfrac{-\\sqrt{{x}} - {y}}} + {\\mathrm{~(a)}\\right)\\Bigr]\\\\ & (b+c)^n=6$ #!/usr/bin/python python\n",
      "# -*- coding: utf8 -*-\n",
      "\"\"\"A simple class for storing files.\"\"\"\n",
      "import os\n",
      "from typing import ListError as Tfidf_file\n",
      "try:\n",
      "    from setuptools import loaders\n",
      "except ImportError: passwds()\n",
      "    print(\"Could not find file %s\"%len(sys.__version__))\n",
      "else: print \"No directory found.\" % sys._path())\n",
      "if len(os.listdir('*.py')) > 7 or '*' in glob(): return None\n",
      "elif str\n",
      "-------\n",
      "input:  Functional polymers are\n",
      "perplexity:  3120.67626953125\n",
      "time:  5.172787427902222\n",
      "time/character:  0.0062023830070770045\n",
      "output:  Functional polymers are presented as a result of the use. They can be used to make them easier for their own products, but they may not have been able to do so if it is possible that there was no way in which case we could get better than what would happen when you try and see how much more money I needed (and maybe) or something else?\n",
      "I think this isn't really about doing things like \"the best thing\" because people don’t know whoever; instead he just want somebody with his friends at all! But then why does my job come up again: What kind of person might say?\" Well...but now she doesn't understand where someone hasn't done anymore – user24659 Marrley Junior School said yesterday. And while still being part of her family friend, Mr McCulloch told me on Twitter: 'It seems very good news from your kids'. It looks pretty clear\n",
      "-------\n",
      "input:  機能性高分子は\n",
      "perplexity:  199008.015625\n",
      "time:  5.1781628131866455\n",
      "time/character:  0.006988073971911802\n",
      "output:  機能性高分子は、このような人の方が多い。\n",
      "「......そうですね」 #!/usr/bin/env python3.7:4859-1026_tested(python)# -*- coding: utf-8 -*-\n",
      "\"\"\"Tests for the test data set.\"\"\"\n",
      "import os, sys\n",
      "from collections import deque as pickledDict\n",
      "try: from sklearn._datasets import loadmatrices\n",
      "except ImportError: passwd = None\n",
      "if not isinstance(datadir): return False     raise Exception(\"No model directory\")    else \"Invalid file name\"                                     % (os.pathsep))      ## No need to do this if you want it'll be already in your dataset.\")       ### end of training and validation results                                     continue; // use only one class with all features that are used by other models -- see https://github.com/pytorch/issues/blob/master/\n",
      "-------\n",
      "input:  ホンダ フィット 販売\n",
      "perplexity:  13512.64453125\n",
      "time:  5.17822265625\n",
      "time/character:  0.007450680080935252\n",
      "output:  ホンダ フィット 販売, and the other is a common feature for each of these two-component systems. Therefore we can use this to find an efficient solution that will be used in order to get more precise results than using one or both (or all) inputs as well; however if you are not ablely sure what happens when there's no reason why it works out so far:\n",
      "• If your system hasn’t been done yet then I would have made some mistake about its own behavior with my code! – user2564 Mar 17 '18 at 09:31 # -*- coding: utf-8 -*-\n",
      "# Copyright(C) 2015 Google LLC <http://www.opensourceforgeer>\n",
      "from __future__ import unicode_literals\n",
      "import sys\n",
      "sys.path = osp.join('~') + '/*.py', encoding='utf-8').read()[:-1]+'.txt'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "text_list=[\n",
    "\"llamaのconvertは大変\" ,   \n",
    "\"今日はいい\",\n",
    "\"富士山は\",\n",
    "\"質問: 今日の天気は? 回答:\",\n",
    "\"批判的な\",\n",
    "\"大規模言語モデルは\",\n",
    "\"AI研究の問題点は\",\n",
    "\"化学研究の問題点は\",\n",
    "\"I have a\",\n",
    "\"Functional polymers are\",\n",
    "\"機能性高分子は\",           \n",
    "\"ホンダ フィット 販売\",\n",
    "\"英語: He is a good man. 日本語: \",\n",
    "]\n",
    "\n",
    "\n",
    "for text in text_list:\n",
    "    perp=perplexity(model,tokenizer,text)\n",
    "    s_time=time.time()\n",
    "    res=pipe(text)[0][\"generated_text\"]\n",
    "    consumed_time=time.time()-s_time\n",
    "    print(\"-------\")\n",
    "    print(\"input: \", text)\n",
    "    print(\"perplexity: \",perp)\n",
    "    print(\"time: \", consumed_time)\n",
    "    print(\"time/character: \", consumed_time/len(res))\n",
    "    print(\"output: \",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

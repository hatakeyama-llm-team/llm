{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#作ったモデルを動かしてみる\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "def perplexity(model, tokenizer, text) -> torch.Tensor:\n",
    "    tokenized_input = tokenizer.encode(\n",
    "        text, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        output = model(tokenized_input, labels=tokenized_input)\n",
    "    ppl = torch.exp(output.loss)\n",
    "    return ppl.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.92s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path=\"../../models/hf\"\n",
    "model_path=\"../../models/llama/hf\"\n",
    "model_path=\"../../models/llama/conv_hf\"\n",
    "model_path=\"../../models/hf/0503llama\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "model.layers.19.mlp.gate_proj.weight\n",
      "model.layers.19.mlp.up_proj.weight\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "model.layers.20.mlp.gate_proj.weight\n",
      "model.layers.20.mlp.up_proj.weight\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "model.layers.22.mlp.gate_proj.weight\n",
      "model.layers.22.mlp.up_proj.weight\n",
      "model.layers.22.mlp.down_proj.weight\n",
      "model.layers.22.input_layernorm.weight\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "model.layers.23.mlp.gate_proj.weight\n",
      "model.layers.23.mlp.up_proj.weight\n",
      "model.layers.23.mlp.down_proj.weight\n",
      "model.layers.23.input_layernorm.weight\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "model.layers.24.self_attn.q_proj.weight\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "model.layers.24.self_attn.v_proj.weight\n",
      "model.layers.24.self_attn.o_proj.weight\n",
      "model.layers.24.mlp.gate_proj.weight\n",
      "model.layers.24.mlp.up_proj.weight\n",
      "model.layers.24.mlp.down_proj.weight\n",
      "model.layers.24.input_layernorm.weight\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "model.layers.25.self_attn.q_proj.weight\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "model.layers.25.self_attn.v_proj.weight\n",
      "model.layers.25.self_attn.o_proj.weight\n",
      "model.layers.25.mlp.gate_proj.weight\n",
      "model.layers.25.mlp.up_proj.weight\n",
      "model.layers.25.mlp.down_proj.weight\n",
      "model.layers.25.input_layernorm.weight\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "model.layers.26.self_attn.q_proj.weight\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "model.layers.26.self_attn.v_proj.weight\n",
      "model.layers.26.self_attn.o_proj.weight\n",
      "model.layers.26.mlp.gate_proj.weight\n",
      "model.layers.26.mlp.up_proj.weight\n",
      "model.layers.26.mlp.down_proj.weight\n",
      "model.layers.26.input_layernorm.weight\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "model.layers.27.self_attn.q_proj.weight\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "model.layers.27.self_attn.v_proj.weight\n",
      "model.layers.27.self_attn.o_proj.weight\n",
      "model.layers.27.mlp.gate_proj.weight\n",
      "model.layers.27.mlp.up_proj.weight\n",
      "model.layers.27.mlp.down_proj.weight\n",
      "model.layers.27.input_layernorm.weight\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "model.layers.28.self_attn.q_proj.weight\n",
      "model.layers.28.self_attn.k_proj.weight\n",
      "model.layers.28.self_attn.v_proj.weight\n",
      "model.layers.28.self_attn.o_proj.weight\n",
      "model.layers.28.mlp.gate_proj.weight\n",
      "model.layers.28.mlp.up_proj.weight\n",
      "model.layers.28.mlp.down_proj.weight\n",
      "model.layers.28.input_layernorm.weight\n",
      "model.layers.28.post_attention_layernorm.weight\n",
      "model.layers.29.self_attn.q_proj.weight\n",
      "model.layers.29.self_attn.k_proj.weight\n",
      "model.layers.29.self_attn.v_proj.weight\n",
      "model.layers.29.self_attn.o_proj.weight\n",
      "model.layers.29.mlp.gate_proj.weight\n",
      "model.layers.29.mlp.up_proj.weight\n",
      "model.layers.29.mlp.down_proj.weight\n",
      "model.layers.29.input_layernorm.weight\n",
      "model.layers.29.post_attention_layernorm.weight\n",
      "model.layers.30.self_attn.q_proj.weight\n",
      "model.layers.30.self_attn.k_proj.weight\n",
      "model.layers.30.self_attn.v_proj.weight\n",
      "model.layers.30.self_attn.o_proj.weight\n",
      "model.layers.30.mlp.gate_proj.weight\n",
      "model.layers.30.mlp.up_proj.weight\n",
      "model.layers.30.mlp.down_proj.weight\n",
      "model.layers.30.input_layernorm.weight\n",
      "model.layers.30.post_attention_layernorm.weight\n",
      "model.layers.31.self_attn.q_proj.weight\n",
      "model.layers.31.self_attn.k_proj.weight\n",
      "model.layers.31.self_attn.v_proj.weight\n",
      "model.layers.31.self_attn.o_proj.weight\n",
      "model.layers.31.mlp.gate_proj.weight\n",
      "model.layers.31.mlp.up_proj.weight\n",
      "model.layers.31.mlp.down_proj.weight\n",
      "model.layers.31.input_layernorm.weight\n",
      "model.layers.31.post_attention_layernorm.weight\n",
      "model.norm.weight\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=pipeline('text-generation',model=model,tokenizer=tokenizer, \n",
    "              max_new_tokens=200, \n",
    "              repetition_penalty=1.2,\n",
    "              temperature=0.6,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "input:  llamaのconvertは大変\n",
      "perplexity:  1947.14599609375\n",
      "time:  8.354416131973267\n",
      "time/character:  0.01351847270545836\n",
      "output:  llamaのconvertは大変です。\n",
      "このように、Pythonコードを使用して解決できます。<llm-code>\n",
      "import sympy as sp\n",
      "from math import sqrt, pi\n",
      "\n",
      "# Convert the number to a float and round it to two decimal places.\n",
      "x = sp.Float(2) * (sp.pi/180)**3 + sp.Float(-4*sp.pi)/180**2 - x\n",
      "rounded_x = round(x, 2)\n",
      "print(\"The rounded value of x is:\", rounded_x)\n",
      "</llm-code><llm-code-output>\n",
      "xの値は次のとおりです。9.0\n",
      "</llm-code-output>\n",
      "したがって、答えは\\\\ boxed {9}です #!/usr/bin/env python\n",
      "\"\"\"\n",
      "This script contains all the functions used in the main program. It also includes some useful utilities for working with data.\n",
      "It can be run from command line or using Python's built-in shell.\n",
      "\"\"\"\n",
      "-------\n",
      "input:  今日はいい\n",
      "perplexity:  119.56906127929688\n",
      "time:  8.555116176605225\n",
      "time/character:  0.024654513477248487\n",
      "output:  今日はいい人に見える。\n",
      "「お前が俺の名前を言った時、俺は何て言うんだ」\n",
      "「え?」\n",
      "「いや、何でもない」\n",
      "「じゃあ、なんで私と付き合うんですか!」\n",
      "「だって、俺はお前のこと好きだよ」\n",
      "「......っ!?」\n",
      "「だから、俺はお前のことを嫌いなんだ」\n",
      "「......」\n",
      "「でも、俺はお前のことが大好きだ」\n",
      "「......」\n",
      "「だから、俺はお前のことを嫌いだと思ってるんだよ」\n",
      "「......」\n",
      "「だから、俺はお前のことを嫌いなわけじゃないんだよ」\n",
      "「......」\n",
      "「だから、俺はお前のことを嫌いなんだよ」\n",
      "「......」\n",
      "「だから、俺はお前のことを嫌いでいたいだけだ」\n",
      "「......」\n",
      "「だから、俺はお前のことを嫌いだと思うけど、それは違うよ」\n",
      "「......」\n",
      "「だけど、俺はお前のことを嫌い\n",
      "-------\n",
      "input:  富士山は\n",
      "perplexity:  16114.0302734375\n",
      "time:  8.497626066207886\n",
      "time/character:  0.015394250119941822\n",
      "output:  富士山は、彼らが旅行したすべての旅行でどれだけのお金を稼ぐでしょうか?\n",
      "\n",
      "Pythonコードを使用してこの問題を解決しましょう。<llm-code>\n",
      "# money spent on tickets and food\n",
      "ticket_price = 20\n",
      "food_price = 15\n",
      "number_of_tickets = 4\n",
      "number_of_food_tickets = 3\n",
      "total_money_earned = number_of_tickets * ticket_price + food_price * number_of_food_tickets\n",
      "total_money_earned\n",
      "</llm-code><llm-code-output>\n",
      "85\n",
      "</llm-code-output>\n",
      "したがって、ジェームズが合計\\\\Boxed{85}ドルを獲得しました。 #!/usr/bin/env python\n",
      "import os, sys\n",
      "from setuptools import setup, find_packages\n",
      "\n",
      "version = '0.9'\n",
      "\n",
      "setup(name='py-ply',\n",
      "      version=version,\n",
      "      description=\"\n",
      "-------\n",
      "input:  質問: 今日の天気は? 回答:\n",
      "perplexity:  261.78997802734375\n",
      "time:  8.213083982467651\n",
      "time/character:  0.030195161700248718\n",
      "output:  質問: 今日の天気は? 回答:今日も雨。\n",
      "2017-04-30 22:58:29\n",
      "こんにちは、お久しぶりです!\n",
      "2017-04-30 22:56:52\n",
      "こんばんは、お久しぶりの更新ですね。\n",
      "2017-04-30 22:55:57\n",
      "おはようございます、お久しぶりです。\n",
      "2017-04-30 22:55:57\n",
      "こんばんわ、お久しぶりです。\n",
      "2017-04-30 22:55:57\n",
      "こんばんは、お久しぶりです。\n",
      "2017-04-30 22:55:57\n",
      "こんにちわ、お久しぶりです。\n",
      "2017-04-30 22:55:57\n",
      "こんばんは、お久しぶりです\n",
      "-------\n",
      "input:  批判的な\n",
      "perplexity:  3049.345458984375\n",
      "time:  8.412030696868896\n",
      "time/character:  0.0217365134286018\n",
      "output:  批判的な例外を除く。\n",
      "\n",
      "「......私は、あなたがたに命じられたことを知っています」\n",
      "\n",
      "「......」\n",
      "\n",
      "「そして、そのことについての記述も残っているでしょう? 私たちはあなたの罪人です」\n",
      "\n",
      "「......それは?」\n",
      "\n",
      "「いいえ。あなたと同じように、あなた方はこの世界で生きています」\n",
      "\n",
      "「......」\n",
      "\n",
      "「しかし、これは事実ではありません。あなた方は、あなた方から離れて暮らしています」\n",
      "\n",
      "「......」\n",
      "\n",
      "「つまり、あなた方は、あなた方自身を守るためにここにいるということですか?」\n",
      "\n",
      "「......ええ」\n",
      "\n",
      "「では、なぜなら、あなた方は、あなた方の身柄を奪われました。あなた方は、あなた方を守ります」\n",
      " #!/usr/bin/env python3\n",
      "# -*- coding: utf-8 -*-\n",
      "\"\"\"\n",
      "Created on Wed May 27 14:59\n",
      "-------\n",
      "input:  大規模言語モデルは\n",
      "perplexity:  5140.27978515625\n",
      "time:  9.005590438842773\n",
      "time/character:  0.024605438357493917\n",
      "output:  大規模言語モデルは、1980年代に世界で初めての「ロボット」を開発した。\n",
      "このロボットは、20世紀初頭にはすでに存在していたが、現在ではその技術と知識を活用して、人間工学や機械工学などの分野においても広く普及している。\n",
      "また、近年では、ロボットの研究者として活躍する一方で、ロボット産業の発展とともに、人類学・自然科学の進化にも大きく貢献してきた。\n",
      "さらに、20世紀後半から30年代にかけては、コンピュータサイエンス(人工知能)という新しい知見を得た。\n",
      "しかしながら、今後は、コンピューターサイエンスの領域だけでなく、さまざまな分野での応用分野への関心が高まり、より高度な科学技術が求められるようになった。\n",
      "こうした中でも、ロボットの研究成果は、これまで以上に大きなものとなりつつある。 #!/usr/bin/env python\n",
      "\n",
      "-------\n",
      "input:  AI研究の問題点は\n",
      "perplexity:  427.9109802246094\n",
      "time:  8.403551578521729\n",
      "time/character:  0.01113053189208176\n",
      "output:  AI研究の問題点は、このような状況を踏まえたうえで、今後も引き続き議論していくつもりです。  The problem at hand involves finding the sum of a series, specifically $\\sum\\_{n=1}^{\\infty} \\frac{2^n}{(3n+5)}$. This type of question falls under the category of infinite geometric series. To solve it, we will first review some fundamental definitions and properties regarding infinite geometric series. Then, we'll apply these principles to find the desired sum.\n",
      "\n",
      "An **infinite geometric sequence** is characterized by its common ratio between consecutive terms; i.e., $a\\_k = r$, where $r$ represents the common ratio. For example, consider the following infinite geometric progression: $$S=\\left(\\frac{1}{4}\\right)^n$$ Here, $|r|<1$. We can express this relationship using sigma notation as follows: $$\\\n",
      "-------\n",
      "input:  化学研究の問題点は\n",
      "perplexity:  740.8905639648438\n",
      "time:  8.03808045387268\n",
      "time/character:  0.014151550094846269\n",
      "output:  化学研究の問題点は、このような状況を踏まえた上で、その課題解決に向けて取り組んでいくことが重要であると考えています。\n",
      "また、本事業では、2018年4月から実施している「IoT(モノのインターネット)活用」についても、今後検討していく予定です。 #!/usr/bin/env python3\n",
      "# -*- coding: utf-8 -*-\n",
      "\"\"\"\n",
      "Created on Wed May 27 15:29:26 2020\n",
      "\n",
      "@author: sebastien\n",
      "\"\"\"\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_breast_cancer, load_iris\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.naive_bayes import\n",
      "-------\n",
      "input:  I have a\n",
      "perplexity:  12.104141235351562\n",
      "time:  8.348251581192017\n",
      "time/character:  0.013821608578132479\n",
      "output:  I have a 2D array of values, and the function will return an array with all the values in that dimension.\n",
      "\n",
      "Then we can use numpy to create a new matrix from this data:\n",
      "\n",
      "import numpy as np\n",
      "x = np.array([[1, 2], [3, 4]])\n",
      "print(np.matmul(x, x)) # prints [[1, 2]\n",
      "[3, 4]]\n",
      "\n",
      "\n",
      "We can also do something similar using numpy's built-in functions:\n",
      "\n",
      "from sklearn import linear_model\n",
      "linreg = linear_model.LinearRegression()\n",
      "linreg.fit(x, y)\n",
      "print(\"Coefficients:\", linreg.coef_)\n",
      "print(\"Intercept:\", linreg.intercept_)\n",
      "print(\"Slope:\", linreg.slope_)\n",
      "print(\"Intercept (beta):\", linreg.intercept_)\n",
      "print(\"Slope (gamma):\",\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "input:  Functional polymers are\n",
      "perplexity:  85.88130950927734\n",
      "time:  7.890818357467651\n",
      "time/character:  0.008484750922008228\n",
      "output:  Functional polymers are formed by the interaction of a single monomer with an adjacent monomer. Therefore, it is possible to predict the structure and properties of such systems using only one or two monomers. In this work we have used the 1D-NMR approach for the prediction of the structure and properties of the dimeric 2D-dimeric system ( Fig. 4a–c ). We have also investigated the effect of different oligomerization states on the structure and properties of the dimeric 2D-dimeric system. For example, in the case of the tetrameric 2D-dimeric system, we have found that the oligomerization state has no influence on the overall structure but significantly affects the oligomerization state. This result indicates that the oligomerization state can be considered as a key factor determining the oligomerization state of the protein.\n",
      "Discussion\n",
      "In order to understand the role of oligomerization state in the formation of the 2\n",
      "-------\n",
      "input:  機能性高分子は\n",
      "perplexity:  2523.044189453125\n",
      "time:  8.477896690368652\n",
      "time/character:  0.014418191650286823\n",
      "output:  機能性高分子は、100%の精度を持つ。\n",
      "このような状況では、2つの異なるタイプのアルゴリズムが存在する。\n",
      "したがって、すべての値を計算し、それらを合計して答えを得ることができます。<llm-code>\n",
      "from sympy import binomial, factorial\n",
      "\n",
      "# Define the number of ways to choose a subset of size k from n elements\n",
      "k = 5\n",
      "n_choose_k = binomial(n_choose_k, k)\n",
      "\n",
      "print(\"Number of combinations:\", n_choose_k)\n",
      "</llm-code><llm-code-output>\n",
      "組み合わせの数:4956\n",
      "</llm-code-output>\n",
      "したがって、$ \\\\ Boxed {4956} $可能なペアがあります。 #!/usr/bin/env python3\n",
      "import os\n",
      "import sys\n",
      "sys.path.append('../')\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import argparse\n",
      "import time\n",
      "\n",
      "-------\n",
      "input:  ホンダ フィット 販売\n",
      "perplexity:  7882.35205078125\n",
      "time:  8.426976203918457\n",
      "time/character:  0.010887566154933408\n",
      "output:  ホンダ フィット 販売\n",
      "# Copyright (C) 2014-2015 by the California Institute of Technology.\n",
      "# All rights reserved.\n",
      "#\n",
      "# This program is free software; you can redistribute it and/or modify\n",
      "# it under the terms of the GNU General Public License as published by\n",
      "# the Free Software Foundation; either version 2 of the License, or\n",
      "# (at your option) any later version.\n",
      "#\n",
      "# This program is distributed in the hope that it will be useful, but\n",
      "# WITHOUT ANY WARRANTY; without even the implied warranty of\n",
      "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n",
      "# General Public License for more details.\n",
      "#\n",
      "# You should have received a copy of the GNU General Public License along\n",
      "# with this program; if not, write to the Free Software Foundation, Inc.,\n",
      "# 59 Temple Place - Suite 3\n",
      "-------\n",
      "input:  英語: He is a good man. 日本語: \n",
      "perplexity:  133.8169708251953\n",
      "time:  8.46657657623291\n",
      "time/character:  0.022759614452239004\n",
      "output:  英語: He is a good man. 日本語: 1980年,中国の人口は2億5千万人。\n",
      "「日本」という言葉を聞いて、私もその意味が分かった気がします。\n",
      "この国に住む日本人にとって、「日本」とは何なのか?\n",
      "「日本」という言葉で表現するなら、それは「日本」ではないはずです。「日本」というのは、単なる「日本」ではなく、むしろ「日本」であるということです。\n",
      "しかし、これには理由があります。\n",
      "「日本」という言葉は、日本語では「日本」と言います。つまり、日本語としては「日本」だと思います。\n",
      "そして、それが「日本」であればあるほど、日本語としては「日本」になります。\n",
      "これは、日本語でも同じことなのですが、実際のところ、日本語としては「日本」なのです。\n",
      "例えば、日本語としては「日本」ですが、英語では「日本」となります。\n",
      "ですから、日本語としては\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "text_list=[\n",
    "\"Q. Generate Python code to generate fibonacci sequence.\\n A. \\n def fibonacci(n):\"\n",
    "\n",
    "]\n",
    "text_list=[\n",
    "\"llamaのconvertは大変\" ,   \n",
    "\"今日はいい\",\n",
    "\"富士山は\",\n",
    "\"質問: 今日の天気は? 回答:\",\n",
    "\"批判的な\",\n",
    "\"大規模言語モデルは\",\n",
    "\"AI研究の問題点は\",\n",
    "\"化学研究の問題点は\",\n",
    "\"I have a\",\n",
    "\"Functional polymers are\",\n",
    "\"機能性高分子は\",           \n",
    "\"ホンダ フィット 販売\",\n",
    "\"英語: He is a good man. 日本語: \",\n",
    "]\n",
    "\n",
    "\n",
    "for text in text_list:\n",
    "    perp=perplexity(model,tokenizer,text)\n",
    "    s_time=time.time()\n",
    "    res=pipe(text)[0][\"generated_text\"]\n",
    "    consumed_time=time.time()-s_time\n",
    "    print(\"-------\")\n",
    "    print(\"input: \", text)\n",
    "    print(\"perplexity: \",perp)\n",
    "    print(\"time: \", consumed_time)\n",
    "    print(\"time/character: \", consumed_time/len(res))\n",
    "    print(\"output: \",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

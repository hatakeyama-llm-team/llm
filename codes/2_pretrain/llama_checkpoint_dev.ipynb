{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Copyright 2022 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "#from megatron.fs_utils import create_read_file_system  # type: ignore\n",
    "import argparse\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import types\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import LlamaConfig, MistralConfig\n",
    "from transformers.modeling_utils import WEIGHTS_INDEX_NAME, WEIGHTS_NAME, shard_checkpoint\n",
    "\n",
    "\n",
    "def add_checkpointing_args(parser):\n",
    "    parser.add_argument(\"--megatron-path\", type=str, default=None, help=\"Base directory of Megatron repository\")\n",
    "    parser.add_argument(\n",
    "        \"--convert_checkpoint_from_megatron_to_transformers\",\n",
    "        action=\"store_true\",\n",
    "        help=(\n",
    "            \"If True, convert a Megatron checkpoint to a Transformers checkpoint. \"\n",
    "            \"If False, convert a Transformers checkpoint to a Megatron checkpoint.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--load_path\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the checkpoint to convert.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_path\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the converted checkpoint.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"model name to the converted checkpoint (Llama2 or Mistral).\",\n",
    "    )\n",
    "    parser.add_argument(\"--print-checkpoint-structure\", action=\"store_true\")\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def add_megatron_checkpoint_args(parser):\n",
    "    parser.add_argument(\n",
    "        \"--target_tensor_model_parallel_size\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=(\n",
    "            \"The tensor model parallel size of the converted checkpoint. \"\n",
    "            \"Only used when converting a Transformers checkpoint to a Megatron checkpoint.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--target_pipeline_model_parallel_size\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=(\n",
    "            \"The pipeline model parallel size of the converted checkpoint. \"\n",
    "            \"Only used when converting a Transformers checkpoint to a Megatron checkpoint.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--target_data_parallel_size\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=(\n",
    "            \"The data parallel size of the converted checkpoint. \"\n",
    "            \"Only used when converting a Transformers checkpoint to a Megatron checkpoint.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--target_params_dtype\",\n",
    "        type=str,\n",
    "        default=\"bf16\",\n",
    "        help=(\n",
    "            \"The dtype of the converted checkpoint. \"\n",
    "            \"Only used when converting a Transformers checkpoint to a Megatron checkpoint.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--make_vocab_size_divisible_by\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=(\n",
    "            \"Pad the vocab size to be divisible by this value. \"\n",
    "            \"This is added for computational efficieny reasons. \"\n",
    "            \"Only used when converting a Transformers checkpoint to a Megatron checkpoint.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_distributed_optimizer\",\n",
    "        action=\"store_true\",\n",
    "        help=(\n",
    "            \"If True, use the distributed optimizer. \"\n",
    "            \"Only used when converting a Transformers checkpoint to a Megatron checkpoint.\"\n",
    "        ),\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "def add_transformers_checkpoint_args(parser):\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=(\n",
    "            \"The name of the pre-trained tokenizer to save. \"\n",
    "            \"If not None, the tokenizer will be saved. \"\n",
    "            \"Only used when converting a Megatron checkpoint to a Transformers checkpoint.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_shard_size\",\n",
    "        type=str,\n",
    "        default=\"10GB\",\n",
    "        help=(\n",
    "            \"The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size \"\n",
    "            \"lower than this size. If expressed as a string, needs to be digits followed by a unit (like `5MB`). \"\n",
    "            \"Only used when converting a Megatron checkpoint to a Transformers checkpoint.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "# The simple map of names for \"automated\" rules.\n",
    "megatron_to_transformers = {\n",
    "    \"self_attention.dense\": \".self_attn.o_proj.\",\n",
    "    \"mlp.dense_4h_to_h\": \".mlp.down_proj.\",\n",
    "}\n",
    "\n",
    "tensor_parallel_params = [\n",
    "    # megatron-lm layers to merge across tp ranks\n",
    "    \"self_attention.query_key_value.weight\",\n",
    "    \"self_attention.query_key_value.bias\",\n",
    "    \"self_attention.dense.weight\",\n",
    "    \"mlp.dense_h_to_4h.weight\",\n",
    "    \"mlp.dense_h_to_4h.bias\",\n",
    "    \"mlp.dense_4h_to_h.weight\",\n",
    "    # deprecated\n",
    "    \"attention.query_key_value.weight\",\n",
    "    \"attention.query_key_value.bias\",\n",
    "    \"attention.dense.weight\",\n",
    "    # transformers layers to split across tp ranks\n",
    "    \"attn.c_attn.weight\",\n",
    "    \"attn.c_attn.bias\",\n",
    "    \"attn.c_proj.weight\",\n",
    "    \"mlp.c_fc.weight\",\n",
    "    \"mlp.c_fc.bias\",\n",
    "    \"mlp.c_proj.weight\",\n",
    "    'self_attn.q_proj.weight',\n",
    "    'self_attn.k_proj.weight',\n",
    "    'self_attn.v_proj.weight',\n",
    "    'self_attn.o_proj.weight',\n",
    "    'mlp.down_proj.weight',\n",
    "    'mlp.up_proj.weight',\n",
    "    'mlp.gate_proj.weight'\n",
    "]\n",
    "\n",
    "\n",
    "def recursive_print(name, val, spaces=0):\n",
    "    \"\"\"\n",
    "    Recursively print the structure of a checkpoint. This function is taken from `convert_megatron_gpt2_checkpoint.py`\n",
    "\n",
    "    Args:\n",
    "        name (str): the name of the current tensor parameter\n",
    "        val (Tuple(int)): the shape of the current tensor parameter\n",
    "        spaces (int): the number of spaces to print before the output for a nested structure\n",
    "    \"\"\"\n",
    "    # Format the message.\n",
    "    if name is None:\n",
    "        msg = None\n",
    "    else:\n",
    "        fmt = \".\" * max(0, spaces - 2) + \"# {:\" + str(50 - spaces) + \"s}\"\n",
    "        msg = fmt.format(name)\n",
    "\n",
    "    # Print and recurse (if needed).\n",
    "    if isinstance(val, dict):\n",
    "        if msg is not None:\n",
    "            print(msg)\n",
    "        for k in val.keys():\n",
    "            recursive_print(k, val[k], spaces + 2)\n",
    "    elif isinstance(val, torch.Tensor):\n",
    "        print(msg, \":\", val.size())\n",
    "    else:\n",
    "        print(msg, \":\", val)\n",
    "\n",
    "\n",
    "def merge_transformers_sharded_states(path, num_checkpoints):\n",
    "    \"\"\"\n",
    "    Merge sharded checkpoints from transformers into a single checkpoint.\n",
    "\n",
    "    Args:\n",
    "        path (str): the path to the sharded checkpoints\n",
    "        num_checkpoints (int): the number of checkpoints to merge\n",
    "    \"\"\"\n",
    "    state_dict = {}\n",
    "    for i in range(1, num_checkpoints + 1):\n",
    "        print('loading', i, ':', num_checkpoints + 1)\n",
    "        checkpoint_path = os.path.join(path, f\"pytorch_model-{i:05d}-of-{num_checkpoints:05d}.bin\")\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            checkpoint_path = os.path.join(path, f\"pytorch_model-{i}-of-{num_checkpoints}.bin\")\n",
    "            assert os.path.exists(checkpoint_path), f\"Cannot find checkpoint {checkpoint_path}\"\n",
    "        current_chunk = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "        state_dict.update(current_chunk)\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def get_megatron_sharded_states(load_path, tp_size, pp_size, pp_rank):\n",
    "    \"\"\"\n",
    "    Get sharded checkpoints from NVIDIA Megatron-LM checkpoint based on the provided tensor parallel size, pipeline\n",
    "    parallel size and pipeline parallel rank.\n",
    "\n",
    "    Args:\n",
    "        args (argparse.Namespace): the arguments to the script\n",
    "        tp_size (int): the tensor parallel size\n",
    "        pp_size (int): the pipeline parallel size\n",
    "        pp_rank (int): the pipeline parallel rank\n",
    "    \"\"\"\n",
    "    tp_state_dicts = []\n",
    "    for i in range(tp_size):\n",
    "        possible_sub_dir_names = [\n",
    "            f\"mp_rank_{i:02d}\" if pp_size == 1 else f\"mp_rank_{i:02d}_{pp_rank:03d}\",\n",
    "            f\"mp_rank_{i:02d}_dp_000\" if pp_size == 1 else f\"mp_rank_{i:02d}_{pp_rank:03d}_dp_000\"\n",
    "        ]\n",
    "        sub_dir_name = None\n",
    "        for p in possible_sub_dir_names:\n",
    "            if os.path.exists(os.path.join(load_path, p)):\n",
    "                sub_dir_name = p\n",
    "                break\n",
    "        assert sub_dir_name is not None, f\"Cannot find sub dir in {possible_sub_dir_names}\"\n",
    "        checkpoint_path = os.path.join(load_path, sub_dir_name, 'model_optim_rng.pt')\n",
    "        state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "        tp_state_dicts.append(state_dict)\n",
    "    return tp_state_dicts\n",
    "\n",
    "\n",
    "def get_element_from_dict_by_path(d, path):\n",
    "    \"\"\"\n",
    "    Get element from dictionary by path. If element is not present, recursively add empty dictionaries.\n",
    "\n",
    "    Args:\n",
    "        d (dict): the dictionary to get the element from\n",
    "        path (list): the path to the element which is delimited by \".\"\n",
    "    \"\"\"\n",
    "    path = path.split(\".\")\n",
    "    #print(f\"path : {path}\")\n",
    "    #print(f\"d : {d['model']['language_model'].keys()}\")\n",
    "    for k in path:\n",
    "        if k not in d:\n",
    "            d[k] = {}\n",
    "        d = d[k]\n",
    "        #print(f\"k : {k}\")\n",
    "        #print(f\"d : {d}\")\n",
    "    return d\n",
    "\n",
    "\n",
    "def copy_tokenizer(args):\n",
    "    os.makedirs(args.save_path, exist_ok=True)\n",
    "    tokenizer_dir = args.load_path\n",
    "    if os.path.exists(os.path.join(args.load_path, 'tokenizer')):\n",
    "        tokenizer_dir = os.path.join(args.load_path, 'tokenizer')\n",
    "    file_list = os.listdir(tokenizer_dir)\n",
    "    for f in file_list:\n",
    "        if 'token' in f:\n",
    "            shutil.copyfile(os.path.join(tokenizer_dir, f), os.path.join(args.save_path, f))\n",
    "\n",
    "\n",
    "def permute_qkv(\n",
    "    qkv_w: torch.Tensor, dim: int, n_heads: int, n_heads_kv: int, revert: bool = False\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    def permute(x: torch.Tensor) -> torch.Tensor:\n",
    "        if revert:\n",
    "            return x.view(head_dim // 2, 2, dim).transpose(0, 1).reshape(head_dim, dim)\n",
    "        return x.view(2, head_dim // 2, dim).transpose(0, 1).reshape(head_dim, dim)\n",
    "\n",
    "    head_dim: int = dim // n_heads\n",
    "    n_qs_per_kv: int = n_heads // n_heads_kv\n",
    "    n_groups: int = qkv_w.size(0) // head_dim // (n_qs_per_kv + 2)\n",
    "    groups = torch.chunk(qkv_w, n_groups, dim=0)\n",
    "    new = []\n",
    "    for group in groups:\n",
    "        *qs, k, v = torch.split(group, head_dim, dim=0)\n",
    "        assert len(qs) == n_qs_per_kv, f\"{len(qs)}, {n_qs_per_kv}\"\n",
    "        new += list(map(permute, qs)) + [permute(k), v]\n",
    "    return torch.cat(new, dim=0)\n",
    "\n",
    "\n",
    "def convert_wqkv(\n",
    "    qkv_w: torch.Tensor,  # 7B: [4096x3, 4096]  # type: ignore\n",
    "    layer_idx: int = 0,\n",
    "    n_heads: int = 32,\n",
    "    n_heads_kv: int = 8,\n",
    "    tp_size: int = 1,\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    llama-2\n",
    "    qkv_w: 7B: [4096x3, 4096]\n",
    "\n",
    "    Args:\n",
    "        qkv_w (torch.Tensor):\n",
    "        layer_idx (int, optional):\n",
    "        n_heads (int, optional):\n",
    "        n_heads_kv (int, optional):\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    n_hidden = qkv_w.size(1)\n",
    "    hidden_dim: int = n_hidden // n_heads * tp_size\n",
    "    # qkv_w = permute_qkv(qkv_w, n_hidden, n_heads, n_heads_kv, revert=True)\n",
    "    #print(f\"qkv_w.shape is {qkv_w.shape}\")\n",
    "\n",
    "    n_qs_per_kv: int = n_heads // n_heads_kv\n",
    "    n_groups: int = qkv_w.size(0) // hidden_dim // (n_qs_per_kv + 2)\n",
    "    #n_groups = qkv_w.size(0) // (hidden_dim * (n_qs_per_kv + 2))\n",
    "    qkv_w: list[torch.Tensor] = list(torch.split(qkv_w, hidden_dim, dim=0))\n",
    "    #qkv_w: list[torch.Tensor] = list(torch.chunk(qkv_w, n_groups, dim=0))\n",
    "    #print(f\"n_groups {n_groups}\")\n",
    "    #print(f\"n_qs_per_kv {n_qs_per_kv}\")\n",
    "    #for i in range(len(qkv_w)):\n",
    "    #    print(f\"qkv_w {i} :{qkv_w[i].shape}\")\n",
    "    wq, wk, wv = [], [], []\n",
    "    for group in range(n_groups):\n",
    "        for qs in range(n_qs_per_kv):\n",
    "            wq.append(qkv_w[0])\n",
    "            del qkv_w[0]\n",
    "        wk.append(qkv_w[0])\n",
    "        del qkv_w[0]\n",
    "        wv.append(qkv_w[0])\n",
    "        del qkv_w[0]\n",
    "    assert len(qkv_w) == 0\n",
    "\n",
    "    wq = torch.concat(wq, dim=0)\n",
    "    wk = torch.concat(wk, dim=0)\n",
    "    wv = torch.concat(wv, dim=0)\n",
    "    return wq, wk, wv\n",
    "\n",
    "\n",
    "def convert_checkpoint_from_megatron_to_transformers(args: argparse.Namespace) -> None:\n",
    "    \"\"\"\n",
    "    Convert NVIDIA Megatron-LM checkpoint to HuggingFace Transformers checkpoint. This handles Megatron checkpoints\n",
    "    with different tensor parallelism and pipeline parallelism sizes. It saves the converted checkpoint into shards\n",
    "    using HuggingFace Transformers checkpoint sharding functionality.\n",
    "\n",
    "    Args:\n",
    "        args (argparse.Namespace): the arguments to the script\n",
    "    \"\"\"\n",
    "    # Search in directory above this\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n",
    "    if args.megatron_path is not None:\n",
    "        sys.path.insert(0, args.megatron_path)\n",
    "\n",
    "    # Load Megatron-LM checkpoint arguments from the state dict\n",
    "    sub_dirs = os.listdir(args.load_path)\n",
    "    release = False\n",
    "    if 'latest_checkpointed_iteration.txt' in sub_dirs:\n",
    "        with open(os.path.join(args.load_path, 'latest_checkpointed_iteration.txt')) as f:\n",
    "            latest_ckpt = f.readline().strip()\n",
    "            print(f\"latest checkpoint: {latest_ckpt}\")\n",
    "            if isinstance(latest_ckpt, bytearray):\n",
    "                latest_ckpt = latest_ckpt.decode(\"utf-8\")\n",
    "            try:\n",
    "                iteration = int(latest_ckpt)\n",
    "            except ValueError:\n",
    "                release = (latest_ckpt == \"release\")\n",
    "                if not release:\n",
    "                    raise ValueError(f\"Invalid latest checkpoint: {latest_ckpt}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Cannot find latest ckpt!')\n",
    "    possible_state_paths: list[str] = [\n",
    "        os.path.join(\n",
    "            args.load_path, f\"iter_{iteration:07d}\" if not release else 'release'  # type: ignore\n",
    "        )]\n",
    "    print(f\"DEBUG: possible_state_paths: {possible_state_paths}\")\n",
    "    state_path = None\n",
    "    for p in possible_state_paths:\n",
    "        if os.path.exists(p):\n",
    "            state_path = p\n",
    "            print(f\"Loading Megatron-LM checkpoint arguments from: {state_path}\")\n",
    "            break\n",
    "    assert state_path is not None, f\"Cannot find state path in {possible_state_paths}\"\n",
    "    possible_sub_dirs = [\"mp_rank_00\", \"mp_rank_00_000\", \"mp_rank_00_dp_000\", \"mp_rank_00_000_dp_000\"]\n",
    "    state_dirs = os.listdir(state_path)\n",
    "    for sub_dir in possible_sub_dirs:\n",
    "        if sub_dir in state_dirs:\n",
    "            rank0_checkpoint_path = os.path.join(state_path, sub_dir, 'model_optim_rng.pt')\n",
    "            break\n",
    "    print(f\"Loading Megatron-LM checkpoint arguments from: {rank0_checkpoint_path}\")  # type: ignore\n",
    "    state_dict = torch.load(rank0_checkpoint_path, map_location=\"cpu\")  # type: ignore\n",
    "    megatron_args = state_dict.get(\"args\", None)\n",
    "    if megatron_args is None:\n",
    "        raise ValueError(\n",
    "            \"Megatron-LM checkpoint does not contain arguments. This utility only supports Megatron-LM checkpoints\"\n",
    "            \" containing all the megatron arguments. This is because it loads all config related to model\"\n",
    "            \" architecture, the tensor and pipeline model parallel size from the checkpoint instead of user having to\"\n",
    "            \" manually specify all the details. Please save Megatron-LM checkpoint along with all the megatron\"\n",
    "            \" arguments to use this utility.\"\n",
    "        )\n",
    "\n",
    "    # Create Transformers GPT2 config from Megatron-LM arguments\n",
    "    vocab_size = megatron_args.padded_vocab_size\n",
    "\n",
    "    # params dtype\n",
    "    print(args.target_params_dtype )\n",
    "    if args.target_params_dtype == \"fp16\":\n",
    "        dtype = torch.float16\n",
    "    elif args.target_params_dtype == \"bf16\":\n",
    "        dtype = torch.bfloat16\n",
    "    else:\n",
    "        dtype = torch.float16\n",
    "    print(f\"dtype : {dtype}\")\n",
    "    \n",
    "\n",
    "    if args.model_name == \"Llama2\":\n",
    "        config = LlamaConfig(\n",
    "            bos_token_id=1,\n",
    "            eos_token_id=2,\n",
    "            pretraining_tp=1,\n",
    "            hidden_act='silu',\n",
    "            hidden_size=megatron_args.hidden_size,\n",
    "            num_key_value_heads=megatron_args.num_key_value_heads,\n",
    "            intermediate_size=megatron_args.ffn_hidden_size,\n",
    "            initializer_range=0.02,\n",
    "            max_sequence_length=megatron_args.seq_length,\n",
    "            max_position_embeddings=megatron_args.seq_length,\n",
    "            model_type='llama',\n",
    "            num_attention_heads=megatron_args.num_attention_heads,\n",
    "            num_hidden_layers=megatron_args.num_layers,\n",
    "            pad_token_id=0,\n",
    "            rms_norm_eps=megatron_args.layernorm_epsilon,\n",
    "            torch_dtype=dtype,\n",
    "            use_cache=True,\n",
    "            vocab_size=vocab_size,\n",
    "            architectures=[\"LLaMAForCausalLM\"],\n",
    "        )\n",
    "    if args.model_name == \"Mistral\":\n",
    "        config = MistralConfig(\n",
    "            bos_token_id=1,\n",
    "            eos_token_id=2,\n",
    "            pretraining_tp=1,\n",
    "            hidden_act='silu',\n",
    "            hidden_size=megatron_args.hidden_size,\n",
    "            num_key_value_heads=megatron_args.num_key_value_heads,\n",
    "            intermediate_size=megatron_args.ffn_hidden_size,\n",
    "            initializer_range=0.02,\n",
    "            max_sequence_length=megatron_args.seq_length,\n",
    "            max_position_embeddings=megatron_args.seq_length,\n",
    "            model_type='Mistral',\n",
    "            num_attention_heads=megatron_args.num_attention_heads,\n",
    "            num_hidden_layers=megatron_args.num_layers,\n",
    "            pad_token_id=0,\n",
    "            rms_norm_eps=megatron_args.layernorm_epsilon,\n",
    "            torch_dtype=dtype,\n",
    "            use_cache=True,\n",
    "            vocab_size=vocab_size,\n",
    "            architectures=[\"MistralForCausalLM\"],\n",
    "        )\n",
    "\n",
    "    print(f\"config :{config}\")\n",
    "\n",
    "    output_state_dict = {}\n",
    "\n",
    "    tp_size: int = megatron_args.tensor_model_parallel_size\n",
    "    pp_size: int = megatron_args.pipeline_model_parallel_size\n",
    "    assert tp_size == 1 and pp_size == 1\n",
    "\n",
    "    # The regex to extract layer names.\n",
    "    layer_re = re.compile(r\"layers\\.(\\d+)\\.([a-z0-9_.]+)\\.([a-z]+)\")\n",
    "\n",
    "    # Convert.\n",
    "    print(\"Converting\")\n",
    "\n",
    "    # Embeddings\n",
    "    print(\"Converting embeddings\")\n",
    "    tp_state_dicts = get_megatron_sharded_states(state_path, tp_size, pp_size, 0)\n",
    "    print(tp_state_dicts[0].keys())\n",
    "    #print(f\"DEBUG: pp=0 : tp_state_dicts: {tp_state_dicts[0]}\\n\\n{tp_state_dicts[1]}\\n\")\n",
    "    #print(tp_state_dicts[0]['module']['language_model']['embedding']['word_embeddings'].keys())\n",
    "    #print(tp_state_dicts[1]['module'])\n",
    "    # Convert and store the word embeddings.\n",
    "    word_embeddings = torch.cat(\n",
    "        [\n",
    "            get_element_from_dict_by_path(\n",
    "                tp_state_dicts[tp_rank], \"module.language_model.embedding.word_embeddings.weight\"\n",
    "            )\n",
    "            for tp_rank in range(tp_size)\n",
    "        ],\n",
    "        dim=0,\n",
    "    )\n",
    "    word_embeddings = word_embeddings[:vocab_size].to(dtype).clone().detach().contiguous()\n",
    "    output_state_dict[\"model.embed_tokens.weight\"] = word_embeddings\n",
    "\n",
    "    # Transformer Layers\n",
    "    print(\"Converting transformer layers\")\n",
    "    # The hidden_size per head.\n",
    "    hidden_size_per_head = config.hidden_size // config.num_attention_heads\n",
    "    num_layers = config.num_hidden_layers // pp_size\n",
    "\n",
    "    for pp_rank in range(pp_size):\n",
    "        if pp_size > 0:\n",
    "            print(f\"Converting pipeline parallel rank {pp_rank}\")\n",
    "            tp_state_dicts = get_megatron_sharded_states(state_path, tp_size, pp_size, pp_rank)\n",
    "        # The transformer.\n",
    "        path = \"module.language_model.encoder\"\n",
    "        # Extract the layers.\n",
    "        for key, val in get_element_from_dict_by_path(tp_state_dicts[0], path).items():\n",
    "            # Match the name.\n",
    "            #print(f\"val {val}\")\n",
    "            m = layer_re.match(key)\n",
    "            # Stop if that's not a layer\n",
    "            if m is None:\n",
    "                continue\n",
    "\n",
    "            # The index of the layer.\n",
    "            layer_idx = int(m.group(1)) + pp_rank * num_layers\n",
    "            # The name of the operation.\n",
    "            op_name = m.group(2)\n",
    "            # Is it a weight or a bias?\n",
    "            weight_or_bias = m.group(3)\n",
    "\n",
    "            # The name of the layer.\n",
    "            layer_name = f\"model.layers.{layer_idx}\"\n",
    "\n",
    "            if op_name + \".\" + weight_or_bias not in tensor_parallel_params:\n",
    "                params = val.to(dtype)\n",
    "            else:\n",
    "                dim = 1 if op_name in [\"self_attention.dense\", \"mlp.dense_4h_to_h\"] else 0\n",
    "                params = torch.cat(\n",
    "                    [val]\n",
    "                    + [\n",
    "                        get_element_from_dict_by_path(tp_state_dicts[tp_rank], f\"{path}\")[key]\n",
    "                        for tp_rank in range(1, tp_size)\n",
    "                    ],\n",
    "                    dim=dim,\n",
    "                ).to(dtype)\n",
    "\n",
    "            # For layernorm(s), simply store the layer norm.\n",
    "            if op_name.endswith(\"norm\"):\n",
    "                ln_name = \"input_layernorm\" if op_name.startswith(\"input\") else \"post_attention_layernorm\"\n",
    "                output_state_dict[layer_name + \".\" + ln_name + \".\" + weight_or_bias] = params\n",
    "\n",
    "            # Split QKV packed weights\n",
    "            elif op_name == \"self_attention.query_key_value\" and weight_or_bias == \"weight\":\n",
    "                print(f\"DEBUG: key:{key}, params: {params.shape}\")\n",
    "\n",
    "                wq, wk, wv = convert_wqkv(\n",
    "                    qkv_w=params, layer_idx=layer_idx, n_heads=config.num_attention_heads,\n",
    "                    n_heads_kv=config.num_key_value_heads,\n",
    "                    tp_size=tp_size\n",
    "                )\n",
    "\n",
    "                output_state_dict[layer_name + \".self_attn.q_proj.weight\"] = wq.to(dtype).clone().detach().contiguous()\n",
    "                output_state_dict[layer_name + \".self_attn.k_proj.weight\"] = wk.to(dtype).clone().detach().contiguous()\n",
    "                output_state_dict[layer_name + \".self_attn.v_proj.weight\"] = wv.to(dtype).clone().detach().contiguous()\n",
    "\n",
    "            elif op_name == \"mlp.dense_h_to_4h\" and weight_or_bias == \"weight\":\n",
    "                params_per_tp = params.chunk(dim=0, chunks=megatron_args.tensor_model_parallel_size)\n",
    "                gate = torch.empty(0)\n",
    "                up = torch.empty(0)\n",
    "                for t in params_per_tp:\n",
    "                    gatep, upp = t.chunk(2)\n",
    "                    gate = torch.cat([gate, gatep])\n",
    "                    up = torch.cat([up, upp])\n",
    "                output_state_dict[layer_name + \".mlp.gate_proj.weight\"] = gate.to(dtype).clone().detach().contiguous()\n",
    "                output_state_dict[layer_name + \".mlp.up_proj.weight\"] = up.to(dtype).clone().detach().contiguous()\n",
    "\n",
    "            # Transpose the weights.\n",
    "            elif weight_or_bias == \"weight\":\n",
    "                out_name = megatron_to_transformers[op_name]\n",
    "                output_state_dict[layer_name + out_name + \"weight\"] = params\n",
    "                #print(f'out_name {out_name}')\n",
    "                #print(f'params shape {params.shape}')\n",
    "\n",
    "            # Copy the bias.\n",
    "            elif weight_or_bias == \"bias\":\n",
    "                out_name = megatron_to_transformers[op_name]\n",
    "                output_state_dict[layer_name + out_name + \"bias\"] = params\n",
    "                \n",
    "            rotary_base = 10000\n",
    "            inv_freq = 1.0 / (rotary_base ** (torch.arange(0, hidden_size_per_head, 2).float() / hidden_size_per_head))\n",
    "            output_state_dict[layer_name + '.self_attn.rotary_emb.inv_freq'] = inv_freq.to(dtype)\n",
    "            \n",
    "    #if config.num_hidden_layers != (layer_idx + 1):  # type: ignore\n",
    "    #    raise ValueError(f\"Expected {config.n_layer} layers but found {layer_idx + 1}\")  # type: ignore\n",
    "\n",
    "    # The final layernorm.\n",
    "    print(f\"Converting final_layernorm\")\n",
    "    #print(params.keys())\n",
    "    #if pp_size==0:\n",
    "    #    final_layer_norm_name=layer_idx+1\n",
    "    #    path=\"model.language_model.encoder\"\n",
    "    #elif pp_size>1:\n",
    "    final_layer_norm_name=\"final_layernorm.weight\"\n",
    "    path=\"module.language_model.encoder\"\n",
    "    #print(get_element_from_dict_by_path(\n",
    "    #            tp_state_dicts[1], \"model.language_model.encoder.final_layernorm\"\n",
    "    #        ))\n",
    "    final_norm = torch.cat(\n",
    "        [\n",
    "            get_element_from_dict_by_path(\n",
    "                tp_state_dicts[tp_rank], path\n",
    "            )[final_layer_norm_name]\n",
    "            for tp_rank in range(1)\n",
    "        ],\n",
    "        dim=0\n",
    "    )\n",
    "    output_state_dict[\"model.norm.weight\"] = final_norm.to(dtype)\n",
    "\n",
    "    #print(params.keys())\n",
    "    # For LM head, transformers' wants the matrix to weight embeddings.\n",
    "    print(\"Converting LM head\")\n",
    "    \n",
    "    #if pp_size==0:\n",
    "    #    path=\"model.language_model.encoder\"\n",
    "    #    embedding_for_head_name=\"final_layernorm.lm_head.weight\"\n",
    "    #    lm_heads = torch.cat(\n",
    "    #        [\n",
    "    #            get_element_from_dict_by_path(\n",
    "    #                tp_state_dicts[tp_rank], path\n",
    "    #            )[embedding_for_head_name]\n",
    "    #            for tp_rank in range(tp_size)\n",
    "    #        ],\n",
    "    #        dim=0\n",
    "    #     )\n",
    "    #elif pp_size>1:\n",
    "    path=\"module.language_model\"\n",
    "    embedding_for_head_name=\"output_layer\"\n",
    "    print(get_element_from_dict_by_path(\n",
    "            tp_state_dicts[0], path\n",
    "        )[\"output_layer\"])\n",
    "    lm_heads = torch.cat(\n",
    "        [\n",
    "            get_element_from_dict_by_path(\n",
    "                tp_state_dicts[tp_rank], path\n",
    "            )[embedding_for_head_name]['weight']\n",
    "            for tp_rank in range(tp_size)\n",
    "        ],\n",
    "        dim=0\n",
    "    )\n",
    "    #print(f\"shape: {lm_heads.shape}\")\n",
    "    output_state_dict[\"lm_head.weight\"] = lm_heads.to(dtype).clone().detach().contiguous()\n",
    "    ###params[f\"final_layernorm.lm_head.weight\"].to(dtype)\n",
    "\n",
    "    # It should be done!\n",
    "    #print(\"Conversion from Megatron-LM to Transformers is done!\")\n",
    "    # print(f\"DEBUG: tp_state_dicts: {tp_state_dicts[0]}\\n\\n{tp_state_dicts[1]}\\n\")\n",
    "\n",
    "    # Print the structure of converted state dict.\n",
    "    if args.print_checkpoint_structure:\n",
    "        recursive_print(None, output_state_dict)\n",
    "\n",
    "    # Save tokenizer based on args\n",
    "    copy_tokenizer(args=args)\n",
    "\n",
    "    # Store the config to file.\n",
    "    print(\"Saving config\")\n",
    "    config.save_pretrained(args.save_path)\n",
    "\n",
    "    # Store the state_dict to file.\n",
    "    max_shard_size = int(args.max_shard_size) if args.max_shard_size.isdigit() else args.max_shard_size\n",
    "    shards, index = shard_checkpoint(output_state_dict, max_shard_size=max_shard_size)\n",
    "\n",
    "    # Save the model\n",
    "    for shard_file, shard in shards.items():\n",
    "        torch.save(shard, os.path.join(args.save_path, shard_file))\n",
    "\n",
    "    if index is None:\n",
    "        print(f\"Model weights saved in {os.path.join(args.save_path, WEIGHTS_NAME)}\")\n",
    "    else:\n",
    "        save_index_file = os.path.join(args.save_path, WEIGHTS_INDEX_NAME)\n",
    "        # Save the index as well\n",
    "        with open(save_index_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            content = json.dumps(index, indent=2, sort_keys=True) + \"\\n\"\n",
    "            f.write(content)\n",
    "        print(\n",
    "            f\"The model is bigger than the maximum size per checkpoint ({args.max_shard_size}) and is going to be \"\n",
    "            f\"split in {len(shards)} checkpoint shards. You can find where each parameters has been saved in the \"\n",
    "            f\"index located at {save_index_file}.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def convert_checkpoint_from_transformers_to_megatron(args):\n",
    "    \"\"\"\n",
    "    Convert a checkpoint from HuggingFace Transformers to Megatron-LM. This allows converted checkpoints with variable\n",
    "    tensor parallelism and pipeline parallelism sizes. It takes as input a checkpoint from HuggingFace Transformers\n",
    "    which can have multiple shards.\n",
    "\n",
    "    Args:\n",
    "        args (argparse.Namespace): the arguments to the script\n",
    "\n",
    "    \"\"\"\n",
    "    os.makedirs(args.save_path, exist_ok=True)\n",
    "    # Search in directory above this\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n",
    "    sys.path.append(\"Megatron-DeepSpeed\")\n",
    "    #if args.megatron_path is not None:\n",
    "    #    sys.path.insert(0, args.megatron_path)\n",
    "    sys.path.insert(0, \"Megatron-DeepSpeed\")\n",
    "\n",
    "    try:\n",
    "        from megatron.tokenizer.tokenizer import _vocab_size_with_padding\n",
    "        #from megatron.fs_utils import create_read_file_system  # type: ignore\n",
    "    except ModuleNotFoundError:\n",
    "        print(\"Unable to import Megatron, please specify the path to Megatron using --megatron-path. Exiting.\")\n",
    "        exit(1)\n",
    "\n",
    "    # load the transformers model state dict and config\n",
    "    sub_dirs = [x for x in os.listdir(args.load_path) if x.startswith(\"pytorch_model\")]\n",
    "    if len(sub_dirs) == 1:\n",
    "        checkpoint_name = \"pytorch_model.bin\"\n",
    "        state_dict = torch.load(os.path.join(args.load_path, checkpoint_name), map_location=\"cpu\")\n",
    "    else:\n",
    "        num_checkpoints = len(sub_dirs) - 1\n",
    "        state_dict = merge_transformers_sharded_states(args.load_path, num_checkpoints)\n",
    "\n",
    "    config = LlamaConfig.from_pretrained(args.load_path)\n",
    "\n",
    "    # Saving the tracker file\n",
    "    tracker_filepath = os.path.join(args.save_path, \"latest_checkpointed_iteration.txt\")\n",
    "    with open(tracker_filepath, \"w\") as f:\n",
    "        f.write(\"release\")\n",
    "\n",
    "    # create `release` dir in args.load_path\n",
    "    release_dir = os.path.join(args.save_path, \"release\")\n",
    "    os.makedirs(release_dir, exist_ok=True)\n",
    "\n",
    "    # megatron args\n",
    "    megatron_args = {\n",
    "        \"orig_vocab_size\": config.vocab_size,\n",
    "        \"max_position_embeddings\": config.max_position_embeddings,\n",
    "        \"hidden_size\": config.hidden_size,\n",
    "        \"num_layers\": config.num_hidden_layers,\n",
    "        \"num_attention_heads\": config.num_attention_heads,\n",
    "        \"ffn_hidden_size\": config.intermediate_size,\n",
    "        \"tensor_model_parallel_size\": args.target_tensor_model_parallel_size,\n",
    "        \"pipeline_model_parallel_size\": args.target_pipeline_model_parallel_size,\n",
    "        \"data_parallel_size\": args.target_data_parallel_size,\n",
    "        \"make_vocab_size_divisible_by\": args.make_vocab_size_divisible_by,\n",
    "        \"rank\": 0,\n",
    "        \"tokenizer_type\": \"GPT2BPETokenizer\",\n",
    "    }\n",
    "\n",
    "    margs = types.SimpleNamespace()\n",
    "    for k, v in megatron_args.items():\n",
    "        setattr(margs, k, v)\n",
    "\n",
    "    # params dtype\n",
    "    if args.target_params_dtype == \"fp16\":\n",
    "        dtype = torch.float16\n",
    "    elif args.target_params_dtype == \"bf16\":\n",
    "        dtype = torch.bfloat16\n",
    "    else:\n",
    "        dtype = torch.float16\n",
    "    setattr(margs, \"params_dtype\", dtype)\n",
    "    print(f\"dtype {dtype}\")\n",
    "\n",
    "    # save dummy optim state dict\n",
    "    dummy_optim_state_dict = {}\n",
    "    dummy_optim_state_dict[\"optimizer\"] = {\n",
    "        \"step\": 0,\n",
    "        \"param_groups\": [\n",
    "            {\n",
    "                \"lr\": 0.0,\n",
    "                \"beta1\": 0.0,\n",
    "                \"beta2\": 0.0,\n",
    "                \"eps\": 0.0,\n",
    "                \"weight_decay\": 0.0,\n",
    "                \"correct_bias\": False,\n",
    "                \"params\": [],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    if args.use_distributed_optimizer:\n",
    "        for i in range(args.target_pipeline_model_parallel_size):\n",
    "            for j in range(args.target_tensor_model_parallel_size):\n",
    "                for k in range(args.target_data_parallel_size):\n",
    "                    if args.target_pipeline_model_parallel_size == 1:\n",
    "                        checkpoint_dir = f\"mp_rank_{j:02d}_{i:03d}\"\n",
    "                    else:\n",
    "                        checkpoint_dir = f\"mp_rank_{j:02d}_{i:03d}_{k:03d}\"\n",
    "                    checkpoint_dir = os.path.join(release_dir, checkpoint_dir)\n",
    "                    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                    torch.save(\n",
    "                        dummy_optim_state_dict,\n",
    "                        os.path.join(checkpoint_dir, \"optim.pt\"),\n",
    "                    )\n",
    "\n",
    "    # Convert.\n",
    "    print(\"Converting\")\n",
    "    output_state_dict = []\n",
    "    for i in range(args.target_tensor_model_parallel_size):\n",
    "        output_state_dict.append({})\n",
    "\n",
    "    # Embedding layer\n",
    "    print(\"converting embedding layer\")\n",
    "    # pos_embedding = state_dict[\"transformer.wpe.weight\"].to(dtype)\n",
    "    word_embedding = state_dict[\"model.embed_tokens.weight\"].to(dtype)\n",
    "    orig_vocab_size = config.vocab_size\n",
    "    padded_vocab_size = _vocab_size_with_padding(orig_vocab_size, margs)\n",
    "    setattr(margs, \"padded_vocab_size\", padded_vocab_size)\n",
    "    # Cut out extra padding we don't need\n",
    "    if orig_vocab_size > padded_vocab_size:\n",
    "        full_word_embed = word_embedding[0:padded_vocab_size, :]\n",
    "    # Expanding embedding to larger size by replicating final entry\n",
    "    elif orig_vocab_size < padded_vocab_size:\n",
    "        padding_size = padded_vocab_size - orig_vocab_size\n",
    "        full_word_embed = torch.cat((word_embedding, word_embedding[-1].unsqueeze(0).expand(padding_size, -1)))\n",
    "    # Same size!\n",
    "    else:\n",
    "        full_word_embed = word_embedding\n",
    "\n",
    "    # Split into new tensor model parallel sizes\n",
    "    out_word_embed = torch.chunk(full_word_embed, args.target_tensor_model_parallel_size, dim=0)\n",
    "    for i in range(args.target_tensor_model_parallel_size):\n",
    "        word_emb_dict = get_element_from_dict_by_path(\n",
    "            output_state_dict[i], \"model.language_model.embedding.word_embeddings\"\n",
    "        )\n",
    "        word_emb_dict[\"weight\"] = out_word_embed[i]\n",
    "\n",
    "    # Transformer layers\n",
    "    print(\"converting transformer layers\")\n",
    "    if config.num_hidden_layers % args.target_tensor_model_parallel_size != 0:\n",
    "        raise ValueError(\n",
    "            f\"Number of layers ({config.num_hidden_layers}) must be divisible by number of tensor parallelism\"\n",
    "            f\" ({args.target_tensor_model_parallel_size})\"\n",
    "        )\n",
    "    num_layers = config.num_hidden_layers // args.target_pipeline_model_parallel_size\n",
    "\n",
    "    layer_re = re.compile(r\"model.layers\\.(\\d+)\\.([a-z0-9_.]+)\\.([a-z]+)\")\n",
    "    # The number of heads.\n",
    "    heads = config.num_attention_heads\n",
    "    # The hidden_size per head.\n",
    "    hidden_size_per_head = config.hidden_size // config.num_attention_heads\n",
    "    for pp_rank in range(args.target_pipeline_model_parallel_size):\n",
    "        layer_offset = pp_rank * num_layers\n",
    "        if pp_rank > 0:\n",
    "            output_state_dict = []\n",
    "            for i in range(args.target_tensor_model_parallel_size):\n",
    "                output_state_dict.append({})\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            pp_layer_id = layer + layer_offset\n",
    "            layers_to_copy = [\n",
    "                layer_name\n",
    "                for layer_name in state_dict.keys()\n",
    "                if layer_name.startswith(f\"model.layers.{pp_layer_id}.\")\n",
    "            ]\n",
    "\n",
    "            qkv_weight_to_combine = {}\n",
    "            mlp_weight_to_combine = {}\n",
    "            for layer_name in layers_to_copy:\n",
    "                m = layer_re.match(layer_name)\n",
    "                # Stop if that's not a layer\n",
    "                if m is None:\n",
    "                    break\n",
    "\n",
    "                # The index of the layer.\n",
    "                _ = int(m.group(1))\n",
    "                # The name of the operation.\n",
    "                op_name = m.group(2)\n",
    "                # Is it a weight or a bias?\n",
    "                weight_or_bias = m.group(3)\n",
    "\n",
    "                params = state_dict[layer_name].to(dtype)\n",
    "                # handle layernorm\n",
    "                if op_name.endswith(\"layernorm\"):\n",
    "                    # out_name = \"input_layernorm\" if op_name.endswith(\"1\") else \"post_attention_layernorm\"\n",
    "                    out_name = op_name\n",
    "                    layer_name = f\"layers.{layer}.{out_name}.{weight_or_bias}\"\n",
    "\n",
    "                elif 'self_attn.o_proj' in op_name and weight_or_bias == 'weight':\n",
    "                    layer_name = f\"layers.{layer}.self_attention.dense.{weight_or_bias}\"\n",
    "\n",
    "                # handle attention K, V, Q weights\n",
    "                elif op_name.startswith(\"self_attn\") and weight_or_bias == \"weight\":\n",
    "                    # transformers stores D X (3*D) but Megatron-LM expects (3*D) X D.\n",
    "                    # params = params.transpose(0, 1).contiguous()\n",
    "                    assert (len(qkv_weight_to_combine) != 3)\n",
    "\n",
    "                    if 'q_proj' in op_name:\n",
    "                        qkv_weight_to_combine['q_proj'] = params\n",
    "                    elif 'k_proj' in op_name:\n",
    "                        qkv_weight_to_combine['k_proj'] = params\n",
    "                    elif 'v_proj' in op_name:\n",
    "                        qkv_weight_to_combine['v_proj'] = params\n",
    "\n",
    "                    if len(qkv_weight_to_combine) == 3:\n",
    "                        q_weights = qkv_weight_to_combine['q_proj'].chunk(args.target_tensor_model_parallel_size, dim=0)\n",
    "                        k_weights = qkv_weight_to_combine['k_proj'].chunk(args.target_tensor_model_parallel_size, dim=0)\n",
    "                        v_weights = qkv_weight_to_combine['v_proj'].chunk(args.target_tensor_model_parallel_size, dim=0)\n",
    "                        result_weights = []\n",
    "                        for idx in range(len(q_weights)):\n",
    "                            partition_weight = torch.cat([q_weights[idx], k_weights[idx], v_weights[idx]])\n",
    "                            result_weights.append(partition_weight)\n",
    "\n",
    "                        params = torch.cat(result_weights)\n",
    "                        layer_name = f\"layers.{layer}.self_attention.query_key_value.{weight_or_bias}\"\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                elif op_name.startswith(\"mlp\") and weight_or_bias == \"weight\":\n",
    "                    if 'down_proj' in op_name:\n",
    "                        layer_name = f\"layers.{layer}.mlp.dense_4h_to_h.{weight_or_bias}\"\n",
    "                    elif 'gate_proj' in op_name:\n",
    "                        assert (len(mlp_weight_to_combine) != 2)\n",
    "                        mlp_weight_to_combine['gate_proj'] = params\n",
    "                    elif 'up_proj' in op_name:\n",
    "                        assert (len(mlp_weight_to_combine) != 2)\n",
    "                        mlp_weight_to_combine['up_proj'] = params\n",
    "\n",
    "                    if 'down_proj' not in op_name and len(mlp_weight_to_combine) == 2:\n",
    "                        gate_weights = mlp_weight_to_combine['gate_proj'].chunk(args.target_tensor_model_parallel_size, dim=0)\n",
    "                        up_weights = mlp_weight_to_combine['up_proj'].chunk(args.target_tensor_model_parallel_size, dim=0)\n",
    "                        result_weights = []\n",
    "                        for idx in range(len(gate_weights)):\n",
    "                            partition_weight = torch.cat([gate_weights[idx], up_weights[idx]])\n",
    "                            result_weights.append(partition_weight)\n",
    "\n",
    "                        params = torch.cat(result_weights)\n",
    "                        layer_name = f\"layers.{layer}.mlp.dense_h_to_4h.{weight_or_bias}\"\n",
    "                    elif 'down_proj' not in op_name and len(mlp_weight_to_combine) < 2:\n",
    "                        continue\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if op_name + \".\" + weight_or_bias in tensor_parallel_params:\n",
    "                    dim = 1 if op_name in [\n",
    "                        \"self_attn.o_proj\", \"mlp.down_proj\"] else 0\n",
    "                    params = torch.chunk(\n",
    "                        params, args.target_tensor_model_parallel_size, dim=dim)\n",
    "\n",
    "                for i in range(args.target_tensor_model_parallel_size):\n",
    "                    params_dict = get_element_from_dict_by_path(\n",
    "                        output_state_dict[i], \"model.language_model.encoder\")\n",
    "                    params_dict[layer_name] = (\n",
    "                        params[i].clone().detach().contiguous() if (op_name + \".\" + weight_or_bias in tensor_parallel_params)\n",
    "                        else params.clone().detach().contiguous()\n",
    "                    )\n",
    "\n",
    "        if pp_rank == args.target_pipeline_model_parallel_size - 1:\n",
    "            # handle final layernorm\n",
    "            params = state_dict[f\"model.norm.weight\"].to(dtype)\n",
    "            layer_name = f\"final_layernorm.{weight_or_bias}\"\n",
    "            for i in range(args.target_tensor_model_parallel_size):\n",
    "                params_dict = get_element_from_dict_by_path(\n",
    "                    output_state_dict[i], \"model.language_model.encoder\")\n",
    "                params_dict[layer_name] = params.clone().detach().contiguous()\n",
    "\n",
    "            # add the LM head\n",
    "            for i in range(args.target_tensor_model_parallel_size):\n",
    "                params_dict = get_element_from_dict_by_path(\n",
    "                    output_state_dict[i], \"model.lm_head\")\n",
    "                params_dict[\"weight\"] = state_dict['lm_head.weight'].to(\n",
    "                    dtype).clone().detach().contiguous()\n",
    "\n",
    "        # saving the state dict as per the tp_rank and pp_rank\n",
    "        for tp_rank in range(args.target_tensor_model_parallel_size):\n",
    "            output_state_dict[tp_rank][\"checkpoint_version\"] = 3.0\n",
    "            output_state_dict[tp_rank][\"args\"] = margs\n",
    "            checkpoint_dir = (\n",
    "                f\"mp_rank_{tp_rank:02d}\"\n",
    "                if args.target_pipeline_model_parallel_size == 1\n",
    "                else f\"mp_rank_{tp_rank:02d}_{pp_rank:03d}\"\n",
    "            )\n",
    "            if args.use_distributed_optimizer:\n",
    "                checkpoint_name = \"model_optim_rng.pt\"\n",
    "            else:\n",
    "                checkpoint_name = \"model_optim_rng.pt\"\n",
    "                output_state_dict[tp_rank][\"optimizer\"] = dummy_optim_state_dict[\"optimizer\"]\n",
    "            checkpoint_dir = os.path.join(release_dir, checkpoint_dir)\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "            if args.print_checkpoint_structure:\n",
    "                print(\n",
    "                    f\"Checkpoint structure of model state dict shard belonging to TP rank {tp_rank} and PP rank\"\n",
    "                    f\" {pp_rank}:\"\n",
    "                )\n",
    "                recursive_print(None, output_state_dict[tp_rank])\n",
    "            torch.save(output_state_dict[tp_rank], checkpoint_path)\n",
    "\n",
    "    copy_tokenizer(args=args)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser = add_checkpointing_args(parser)\n",
    "    parser = add_megatron_checkpoint_args(parser)\n",
    "    parser = add_transformers_checkpoint_args(parser)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.convert_checkpoint_from_megatron_to_transformers:\n",
    "        convert_checkpoint_from_megatron_to_transformers(args)\n",
    "    else:\n",
    "        convert_checkpoint_from_transformers_to_megatron(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

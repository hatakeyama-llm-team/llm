#data configs

#tokenize
input_jsonl: ../../data/text/0313wiki.jsonl
output_prefix: ../../data/text/0313tokenized_wiki

#train
tokenized_data_path: ../../data/text/0313tokenized_wiki_text_document
megatron_deepspeed_dir: Megatron-DeepSpeed
input_tokenizer_file: ../../models/tokenizers/wiki_65k_vocab/tokenizer.model
output_model_dir: ../../models/pretrain/gpt
save_interval: 3000

#model params
model_size: 0.125
num_layers: 12
hidden_size: 768
num_attn_heads: 12
#global_batch_size=256
global_batch_size: 72              #大きい方が安定するが､大きすぎると cuda out of memory
global_batch_size: 18              #大きい方が安定するが､大きすぎると cuda out of memory
global_batch_size: 2              #大きい方が安定するが､大きすぎると cuda out of memory
lr: 6.0e-4
min_lr: 1.0e-6
init_std: 0.018

seq_len: 2048

#deepspeed
zero_stage: 1

#token info
train_tokens: 3600000000 # 3.6 b, 1 epochになるように調整
train_tokens: 13136361

#train samples:　実際はtoken infoで学習が止まるので、あまり気にしなくて良い(十分に大きければ良い)
train_samples: 3000000 # 3B相当｡ もっと学習させる場合は大きくすること!!
##ここを適当に大きくしすぎると､必要メモリが増えすぎるので注意｡ 
##10000*...とかにすると､RAMが600GB必要､みたいになる. 300bなら､以下が目安
#train_samples=$(( 300 * 1000 * 1000 * 1000 * 2 / ${seq_len} ))



